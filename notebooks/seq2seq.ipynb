{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-transformers/blob/main/exercise/exercise_01.ipynb)\n",
    "\n",
    "![](https://cdn.britannica.com/03/134503-050-060DD73F/Bombe-American-version-messages-cipher-machines-Britain.jpg)\n",
    "\n",
    "In this notebook, we will be creating a seq2seq model for deciphering a simple cipher. \n",
    "References: \n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Colab or local environments, install the following packages:\n",
    "#!pip install spacy\n",
    "#!pip install torchtext\n",
    "#!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pyl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "Let us implement a seq2seq model with attention mechanism. We will first implement its building blocks, namely `Encoder`, `Decoder`, and `Attention`, and then put them together to form a seq2seq model.\n",
    "\n",
    "\n",
    "## Encoder \n",
    "\n",
    "Let's implement `Encoder`. \n",
    "While [the original paper uses four-layer LSTM](https://arxiv.org/abs/1409.3215), we will cut down it to simpler encoder, namely two-layer [Gated Recurrent Unit (GRU) by Cho et al.](https://arxiv.org/pdf/1406.1078v3.pdf). GRU simplifies LTCM by omitting the cell state and produces only the hidden state. Namely, \n",
    "\n",
    "$$\n",
    "h_{t} = \\text{GRU}(x_{t}, h_{t-1})\n",
    "$$\n",
    "\n",
    "*Multi-layered* GRU means that GRU units are stacked on top of each other, where $\\ell$th ($\\ell \\geq 2$) GRU will take $\\ell-1$th GRU's hidden state as the input. For example, two-layer GRU is given by \n",
    "\n",
    "$$\n",
    "h^{(1)}_{t} = \\text{GRU}(x_{t}, h^{(1)}_{t-1}) \\\\\n",
    "h^{(2)}_{t} = \\text{GRU}(h^{(1)}_{t}, h^{(2)}_{t-1})\n",
    "$$\n",
    "\n",
    "where $h^{(\\ell)}_t$ represents the hidden state for the $\\ell$ th layer at the time $t$. We will then use all layer's hidden states at the end of the sequence as the inputs to the decoder. \n",
    "With PyTorch, we can easily implement the multi-layer GRU. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n",
    "\n",
    "Here, let's implement `Encoder` class as follows. \n",
    "\n",
    "**Step 1**: `Encoder` will take sequences of integer tokens, represented as a tensor of size <batch_size x max_length>, where `batch_size` is the number of sentences in a batch, and `max_length` is the maximum length of the sentences in the batch. \n",
    "\n",
    "**Step 2**: The integer tokens are mapped to the vectors of size `embedding_size` by using `torch.nn.Embedding`, namely\n",
    "$$\n",
    "z_t = \\text{Embedding}(x_t)\n",
    "$$\n",
    "where $z_t$ is the vector representation of the token $x_t$. \n",
    "\n",
    "**Step 3**: A dropout is performed on $z_t$:\n",
    "\n",
    "$$\n",
    "z_t = \\text{Dropout}(z_t )\n",
    "$$\n",
    "\n",
    "Dropout is a technique to prevent overfitting by randomly dropping out some neurons during training. For example, if we set the dropout rate to 0.5, then 50% of the neurons will output zeros randomly during the training.  Neural networks with dropout tend to avoid relying on specific neurons, and instead learn robust mapping between the input and output. \n",
    "\n",
    "**Step 4**: Embedding $z_t$ will be fed into the two-layer GRUs:\n",
    "$$\n",
    "h^{(1)}_t = \\text{GRU}(z_t, h^{(1)}_{t-1}) \\\\\n",
    "h^{(2)}_t = \\text{GRU}(h^{(1)}_t, h^{(2)}_{t-1})\n",
    "$$\n",
    "\n",
    "**Step 5**: Output the hidden states at the last sequence time $T$, namely \n",
    "\n",
    "$$\n",
    "(h^{(1)}_T, h^{(2)}_T)\n",
    "$$\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    input[/\"Input Sequence\n",
    "    (batch_size Ã— max_length)\"/]\n",
    "    embed[\"Embedding Layer\n",
    "    zt = Embedding(xt)\"]\n",
    "    dropout[\"Dropout Layer\"]\n",
    "    gru[\"Two-Layer GRU\n",
    "    h1(t), h2(t) = GRU(zt)\"]\n",
    "    output[/\"Final Hidden States\n",
    "    (h1(T), h2(T)) and Outputs\"/]\n",
    "    input --> embed\n",
    "    embed --> dropout\n",
    "    dropout --> gru\n",
    "    gru --> output\n",
    "\n",
    "    style input fill:#D4E6F1,stroke:#000,color:#000\n",
    "    style embed fill:#FAE5D3,stroke:#000,color:#000\n",
    "    style dropout fill:#D5F5E3,stroke:#000,color:#000\n",
    "    style gru fill:#E8DAEF,stroke:#000,color:#000\n",
    "    style output fill:#FADBD8,stroke:#000,color:#000\n",
    "\n",
    "    linkStyle default stroke:#000,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, n_layers=2, dropout=0.1, bidirectional=False\n",
    "    ):\n",
    "        \"\"\"Encoder class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            The number of unique tokens in the input sequence\n",
    "        embedding_size: int\n",
    "            The dimension of the embedding vectors\n",
    "        hidden_size: int\n",
    "            The dimension of the hidden states\n",
    "        n_layers: int\n",
    "            The number of layers in the GRU\n",
    "        dropout: float\n",
    "            The dropout rate\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # TODO:\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize the embedding\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        hidden: Tensor of shape <batch_size x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        Z = self.embedding(X)\n",
    "        Z = self.dropout(Z)\n",
    "        outputs, hidden = self.gru(Z)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention \n",
    "\n",
    "Let's implement attention module. This module takes two inputs: \n",
    "- the hidden states of the encoder, $h_{1}, \\ldots, h_{n}$\n",
    "- the hidden states of the decoder, $s_{t-1}$\n",
    "\n",
    "These states are concatenated and fed into an MLP to generate the attention scores, $e_{1t}, e_{2t}, \\ldots, e_{nt}$, i.e., \n",
    "\n",
    "$$\n",
    "e_{it} = \\text{MLP}([h_i, s_{t-1}])\n",
    "$$\n",
    "\n",
    "where $i$ is the index of the encoder hidden states.  These scores are then normalized by the softmax function to generate the attention weights, $\\alpha_{1t}, \\alpha_{2t}, \\ldots, \\alpha_{nt}$, i.e., \n",
    "\n",
    "$$\n",
    "\\alpha_{it} = \\frac{\\exp(e_{it})}{\\sum_{j=1}^{n} \\exp(e_{jt})}\n",
    "$$\n",
    "\n",
    "Finally, a new context vector is generated by taking the  weighted average: \n",
    "\n",
    "$$\n",
    "c_t = \\sum_{i=1}^{n} \\alpha_{it} h_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_decoder_hidden_size, attention_hidden_size, n_layers_hidden, bidirectional = False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_decoder_hidden_size = encoder_decoder_hidden_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.n_layers_hidden = n_layers_hidden\n",
    "        self.bidirectional = 2 if bidirectional else 1\n",
    "        self.enc2hidden = torch.nn.Linear(encoder_decoder_hidden_size * self.bidirectional, attention_hidden_size)\n",
    "        self.dec2hidden = torch.nn.Linear(encoder_decoder_hidden_size * self.bidirectional * self.n_layers_hidden, attention_hidden_size)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.hidden2score = torch.nn.Linear(attention_hidden_size, 1)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_outputs: Tensor of shape <batch_size x seq_len x output_size>\n",
    "        decoder_hidden: Tensor of shape <batch_size x 1 x (n_layers*hidden_size)>\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Project encoder hidden states\n",
    "        enc_proj = self.enc2hidden(encoder_outputs)  # [batch x seq_len x hidden]\n",
    "\n",
    "        # Project decoder hidden state\n",
    "        # Reshape decoder hidden from (x, batch_size, hidden_dim) to (batch_size, x*hidden_dim)\n",
    "        concat_hidden = decoder_hidden.permute(1, 0, 2)  # (batch_size, x, hidden_dim)\n",
    "        concat_hidden = concat_hidden.reshape(batch_size, -1)  # (batch_size, x*hidden_dim)\n",
    "        concat_hidden = concat_hidden.unsqueeze(1)  # (batch_size, 1, x*hidden_dim)\n",
    "        dec_proj = self.dec2hidden(concat_hidden)   # [batch x 1 x hidden]\n",
    "        #dec_proj = dec_proj.expand(-1, seq_len, -1)  # Expand to match encoder sequence length\n",
    "\n",
    "        # Combine and get attention scores\n",
    "        hidden = enc_proj + dec_proj\n",
    "        hidden = self.activation(hidden)\n",
    "        scores = self.hidden2score(hidden)  # [batch x seq_len x 1]\n",
    "        scores = self.softmax(scores)\n",
    "\n",
    "        # Get context vector via weighted sum\n",
    "        context_vector = torch.bmm(scores.transpose(1,2), encoder_outputs)  # [batch x 1 x hidden]\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder \n",
    "\n",
    "Let's implement `Decoder`. Following the `Encoder`, we will simplify the original implementation by using two-layer GRUs. \n",
    "\n",
    "The input to the decoder are\n",
    "- The hidden states of the encoder, $h_{1}, \\ldots, h_{n}$\n",
    "- The hidden state of the decoder at the previous time step, $s_{t-1}$\n",
    "- The previous token, $x_{t-1}$\n",
    "\n",
    "The decoder first computes the context vector, $c_t$, by using the attention mechanism. \n",
    "\n",
    "$$\n",
    "c_t = \\text{Attention}(h_1, \\ldots, h_n, s_{t-1})\n",
    "$$\n",
    "\n",
    "Apply dropout to $c_t$ and concatenate it with the embedding of the previous token, $x_{t-1}$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_t &= [\\text{Dropout}(c_t), \\text{Embedding}(x_{t-1})]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We then concatenate the context vector and the embedding of the previous token, and feeds them into the GRU:\n",
    "\n",
    "$$\n",
    "s_t = \\text{GRU}\\left(z_t, s_t\\right)\n",
    "$$\n",
    "\n",
    "where $\\text{Embedding}$ is the embedding layer that maps the previous token to the embedding vector. Finally, the decoder outputs the probability distribution of the next token, $P(x_t \\vert x_0, \\ldots, x_{t-1})$ using a linear layer based on the hidden state $s_t$.  \n",
    "\n",
    "$$\n",
    "P(x_t \\vert x_0, \\ldots, x_{t-1}) = \\text{Linear}(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embedding_size,\n",
    "        readout_hidden_size,\n",
    "        encoder_decoder_hidden_size,\n",
    "        attention_hidden_size,\n",
    "        output_size,\n",
    "        n_layers = 2,\n",
    "        dropout=0.1,\n",
    "        bidirectional = False\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.encoder_decoder_hidden_size = encoder_decoder_hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.readout_hidden_size = readout_hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding layer to convert input tokens to vectors\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # GRU that takes concatenated context vector and embedded input\n",
    "        self.gru = torch.nn.GRU(\n",
    "            embedding_size + encoder_decoder_hidden_size,  # Input size is embedding + context vector\n",
    "            encoder_decoder_hidden_size,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(\n",
    "            encoder_decoder_hidden_size=encoder_decoder_hidden_size,\n",
    "            attention_hidden_size=attention_hidden_size,\n",
    "            n_layers_hidden=n_layers,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Output layer to predict next token\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoder_decoder_hidden_size, readout_hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(readout_hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "        # Softmax for converting outputs to probabilities\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "        # Initialize embeddings\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, input_tokens, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x 1>\n",
    "            The input sequence\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            The hidden states of the decoder from previous timestep\n",
    "        encoder_outputs: Tensor of shape <batch_size x seq_len x hidden_size>\n",
    "            The hidden states from the encoder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: Tensor of shape <batch_size x 1 x output_size>\n",
    "            Probability distribution over next token\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            Updated decoder hidden states\n",
    "        \"\"\"\n",
    "\n",
    "        # Get context vector using attention\n",
    "        context_vector = self.attention(encoder_outputs, hidden)\n",
    "        context_vector = self.dropout(context_vector)\n",
    "\n",
    "        # Embed input tokens and ensure it has batch dimension\n",
    "        input_vector = self.embedding(input_tokens).unsqueeze(1) if input_tokens.dim() == 1 else self.embedding(input_tokens)\n",
    "\n",
    "        # Concatenate context vector and embedded input\n",
    "        z = torch.cat([context_vector, input_vector], dim=2)\n",
    "\n",
    "        # Pass through GRU\n",
    "        output, hidden = self.gru(z, hidden)\n",
    "\n",
    "        # Generate output probabilities\n",
    "        output = self.fc(output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq \n",
    "\n",
    "Now, let's put them together to build a seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "class Seq2Seq(LightningModule):\n",
    "\n",
    "    def __init__(self, encoder, decoder, sos_token_id, eos_token_id, vocab_size, teacher_forcing_ratio=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sos_token_id = sos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, input_tokens, max_output_len, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "        max_output_len: int\n",
    "            The maximum length of the output sequence\n",
    "        \"\"\"\n",
    "        batch_size = input_tokens.size(0)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Get encoder outputs and hidden states\n",
    "        encoder_outputs, encoder_hiddens = self.encoder(input_tokens)\n",
    "        decoder_hidden = encoder_hiddens\n",
    "\n",
    "        # First input to decoder is SOS token\n",
    "        decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.sos_token_id\n",
    "\n",
    "        # Initialize outputs tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, max_output_len, device=self.device)\n",
    "\n",
    "        # Generate sequence\n",
    "        for t in range(max_output_len):\n",
    "            output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            outputs[:, t] = torch.argmax(output, dim=2)\n",
    "\n",
    "            # Sample next token using temperature\n",
    "            if temperature > 0:\n",
    "                probs = torch.nn.functional.softmax(output.squeeze(1) / temperature, dim=-1)\n",
    "                decoder_input = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "            # Stop if all sequences in batch hit EOS\n",
    "            if (decoder_input == self.eos_token_id).all():\n",
    "                break\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "\n",
    "        # Teacher forcing: use the actual target tokens as input to decoder\n",
    "        batch_size = src.size(0)\n",
    "        target_length = trg.size(1)\n",
    "        vocab_size = self.decoder.fc[-1].out_features\n",
    "\n",
    "        # Initialize outputs tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_length, vocab_size)\n",
    "\n",
    "        # Get initial decoder hidden state from encoder\n",
    "        encoder_outputs, encoder_hiddens = self.encoder(src)\n",
    "        decoder_hidden = encoder_hiddens\n",
    "\n",
    "        # First input to decoder is SOS token\n",
    "        decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.sos_token_id\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        loss = 0\n",
    "        for t in range(target_length):\n",
    "            output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            if np.random.rand() < self.teacher_forcing_ratio:\n",
    "                decoder_input = trg[:, t].unsqueeze(1)  # Use target token as next input\n",
    "            else:\n",
    "                decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "            loss += self.loss_fn(output.squeeze(1), trg[:, t])\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            src, trg = batch\n",
    "\n",
    "            # Teacher forcing: use the actual target tokens as input to decoder\n",
    "            batch_size = src.size(0)\n",
    "            target_length = trg.size(1)\n",
    "            vocab_size = self.vocab_size\n",
    "\n",
    "            # Initialize outputs tensor to store decoder outputs\n",
    "            outputs = torch.zeros(batch_size, target_length, vocab_size)\n",
    "\n",
    "            # Get initial decoder hidden state from encoder\n",
    "            encoder_outputs, encoder_hiddens = self.encoder(src)\n",
    "            decoder_hidden = encoder_hiddens  # Add layer dimension\n",
    "\n",
    "            # First input to decoder is SOS token\n",
    "            decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.sos_token_id\n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            loss = 0\n",
    "            for t in range(target_length):\n",
    "                output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                if np.random.rand() < self.teacher_forcing_ratio:\n",
    "                    decoder_input = trg[:, t].unsqueeze(1)  # Use target token as next input\n",
    "                else:\n",
    "                    decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "                loss += self.loss_fn(output.squeeze(1), trg[:, t])\n",
    "\n",
    "            self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['qnfsxpoiiz', 'mhyicduhcj', 'jemdbbsrxc']\n",
      "Ciphered: ['trkyexxstl', 'pldojldrnv', 'mirjijbbio']\n"
     ]
    }
   ],
   "source": [
    "from secretpy import Caesar, CaesarProgressive, alphabets as al\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def generate_random_sequences(cipher_key, cipher, n_seqs, seq_len):\n",
    "\n",
    "    sents = []\n",
    "    ciphered_sents = []\n",
    "    for _ in range(n_seqs):\n",
    "        sequence = \"\".join(random.choices(string.ascii_lowercase, k=seq_len))\n",
    "        ciphered_sequence = cipher.encrypt(sequence, cipher_key, al.ENGLISH)\n",
    "        # assert len(sequence) == len(ciphered_sequence)\n",
    "        assert sequence == cipher.decrypt(ciphered_sequence, cipher_key)\n",
    "        sents.append(sequence)\n",
    "        ciphered_sents.append(ciphered_sequence)\n",
    "    return ciphered_sents, sents\n",
    "\n",
    "\n",
    "key = 3\n",
    "cipher = CaesarProgressive()\n",
    "ciphered_sents, sents = generate_random_sequences(\n",
    "    cipher_key=key, cipher=cipher, n_seqs=10000, seq_len=10\n",
    ")\n",
    "\n",
    "print(\"Original:\", sents[:3])\n",
    "print(\"Ciphered:\", ciphered_sents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 15, 11, 3, 14, 9, 11, 3, 17, 13, 21, 27]\n",
      "[26, 7, 3, 5, 9, 15, 2, 25, 8, 4, 23, 27] [26, 12, 7, 24, 8, 2, 3, 20, 7, 2, 9, 27]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_tokenizer(sents):\n",
    "    vocab = sorted(list(set(\"\".join(sents))))\n",
    "\n",
    "    vocab.append(\"<sos>\")  # <sos> token\n",
    "    vocab.append(\"<eos>\")  # <eos> token\n",
    "    vocab.append(\"<unk>\")  # <unk> token used to represent the unknown token\n",
    "\n",
    "    vocab_stoi = {token: i for i, token in enumerate(vocab)}\n",
    "    vocab_itos = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "    sos_token_id = vocab_stoi[\"<sos>\"]\n",
    "    eos_token_id = vocab_stoi[\"<eos>\"]\n",
    "    unk_token_id = vocab_stoi[\"<unk>\"]\n",
    "\n",
    "    # If the token is not in the vocabulary, then return the unk_token_id\n",
    "    # vocab_stoi = defaultdict(lambda: unk_token_id, vocab_stoi)\n",
    "    # vocab_itos = defaultdict(lambda: unk_token_id, vocab_itos)\n",
    "\n",
    "    return {\n",
    "        \"stoi\": vocab_stoi,\n",
    "        \"itos\": vocab_itos,\n",
    "        \"sos_token_id\": sos_token_id,\n",
    "        \"eos_token_id\": eos_token_id,\n",
    "        \"unk_token_id\": unk_token_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize(sents, vocab):\n",
    "    retval = []\n",
    "    for sent in sents:\n",
    "        _retval = [vocab[\"sos_token_id\"]]\n",
    "        for letter in sent:\n",
    "            _retval.append(vocab[\"stoi\"][letter])\n",
    "        _retval.append(vocab[\"eos_token_id\"])\n",
    "        retval.append(_retval)\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "src_vocab = build_tokenizer(ciphered_sents)\n",
    "trg_vocab = build_tokenizer(sents)\n",
    "\n",
    "src_tokenized = tokenize(ciphered_sents, src_vocab)\n",
    "trg_tokenized = tokenize(sents, trg_vocab)\n",
    "\n",
    "print(src_tokenized[1])\n",
    "print(trg_tokenized[3], trg_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeloine\n",
    "batch_size = 100\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(src_tokenized, dtype=torch.long),\n",
    "    torch.tensor(trg_tokenized, dtype=torch.long),\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_src_vocab = len(src_vocab[\"stoi\"]) + 3\n",
    "n_trg_vocab = len(trg_vocab[\"stoi\"]) + 3\n",
    "\n",
    "n_layers = 2 # number of layers in the GRU\n",
    "bidirectional = False # whether to use bidirectional GRU\n",
    "embedding_size = 32 # embedding size\n",
    "hidden_size = 16 # hidden size\n",
    "dropout = 0.1 # dropout rate\n",
    "readout_hidden_size = 16 # hidden size of the readout layer\n",
    "attention_hidden_size = 16 # hidden size of the attention layer\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size=n_src_vocab,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    bidirectional=bidirectional\n",
    ")\n",
    "decoder = Decoder(\n",
    "    input_size=n_trg_vocab,\n",
    "    embedding_size=embedding_size,\n",
    "    readout_hidden_size=readout_hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    output_size=n_trg_vocab,\n",
    "    dropout=dropout,\n",
    "    encoder_decoder_hidden_size=hidden_size,\n",
    "    attention_hidden_size=attention_hidden_size,\n",
    "    bidirectional=bidirectional\n",
    ")\n",
    "sos_token_id = trg_vocab[\"sos_token_id\"]\n",
    "eos_token_id = trg_vocab[\"eos_token_id\"]\n",
    "model = Seq2Seq(encoder, decoder, sos_token_id, eos_token_id, vocab_size=n_trg_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Let us implement a trainer for the seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 60521), started 0:18:14 ago. (Use '!kill 60521' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6e5549483fa91f49\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6e5549483fa91f49\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 5.1 K  | train\n",
      "1 | decoder | Decoder          | 7.5 K  | train\n",
      "2 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1804bac0fd4e419d91fca657ca7b7b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002ce425ea89400ea19a1192e25b3d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84651eafff4e60af1255dd280c944b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4385ba82012b46088ef29557580edd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362eee512f90455e8bf31745356d4ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ffd4ec4de34e2ebf65f4b766193305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c9d2e645914742a54e504a63112f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4144b4d0fb1c4dfabe426fbf00cfaf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d64d396ad8f4e858457b68ae869ce89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9b92c8baee4f59a6fdca91611713e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb18d3efbaa646c09913b67505e16db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346613fb30c4447793b16f8991f14d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c817868485e4c93b711104f7bb278e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83543261185648deae8c36d9cc2714fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54d903e46b7434bb1d17080702fa145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daa29b54b564462b7e21901e750ab52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c7fac87d7e4d4998e52ba0d742bcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9dfa24f7aa402b80a0416e3a450091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93192a5880cd4b63bf2db43d7dcb4c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47064a5c64624d85b8c1bce1121d075a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61816f97b0074dfcb937dadb588a055b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71563139e597462ca516b9bb74de7e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7382fac54e4727929a13c88adeb28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704abc6ed5ca47ac9d2239d2d0561245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96283d60b8ea46c18efb4276b75ee6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753b766296e743f999b65865527e6dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69277c9719354a0187cd48c79a18b393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8118d38a4ce4dc2bd4b53cb62d193a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11ea985ee8d402c927d84d895ffaaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3a1ab94d742caa6fe9037ee502890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a568e33508c4b9e97b100ec0b6c4af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7737d24f1cf140f6939b588b834a7a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7904b0f58c34eb5bf5f896f68e1b2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fa2e7123fc403eaa83aeadc1524ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6fe7c5de3241efa5d1483d522947e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"logs\", name=\"seq2seq\")\n",
    "\n",
    "trainer = Trainer(max_epochs=100, logger=logger)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Let's validate the seq2seq model with [Caesar cipher](https://en.wikipedia.org/wiki/Caesar_cipher). \n",
    "We will generate ciphered texts to train seq2seq and see if the trained seq2seq decipher the text correctly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you decipher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  0  0  8  8  8 17 17 17 27 27]]\n",
      "Original-Deciphered: esatvlyspx  <-->  <sos>aaaiiirrr<eos><eos>\n",
      "[[26  0  0  0  8  8  8 17 17 17 27 27]]\n",
      "Original-Deciphered: fnvjgkfwyi  <-->  <sos>aaaiiirrr<eos><eos>\n",
      "[[26  0  0  0  8  8 18 17 17  3 27 27]]\n",
      "Original-Deciphered: kpjyemthcy  <-->  <sos>aaaiisrrd<eos><eos>\n",
      "[[26  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "Original-Deciphered: jkzzjqomre  <-->  <sos>aaaaaaaaaaa\n",
      "[[26  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "Original-Deciphered: lgrbctcdfs  <-->  <sos>aaaaaaaaaaa\n",
      "[[26  0  0  0  8  8  8 17 17 17 27 27]]\n",
      "Original-Deciphered: symvyfciip  <-->  <sos>aaaiiirrr<eos><eos>\n",
      "[[26  0  0  0  4  8  8 17 17 17 27 27]]\n",
      "Original-Deciphered: wniaporagv  <-->  <sos>aaaeiirrr<eos><eos>\n",
      "[[26  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "Original-Deciphered: tfjwbqperh  <-->  <sos>aaaaaaaaaaa\n",
      "[[26  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "Original-Deciphered: fwlzgzzaky  <-->  <sos>aaaaaaaaaaa\n",
      "[[26  0  0  0  8  8  8 17 17 17 27 27]]\n",
      "Original-Deciphered: wlvidnmaha  <-->  <sos>aaaiiirrr<eos><eos>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# text = \"iamastudent\"\n",
    "for i in range(10):\n",
    "    text = sents[i]\n",
    "    ciphered_text = cipher.encrypt(text, key)\n",
    "    ciphered_text_tokenized = torch.tensor(tokenize([ciphered_text], src_vocab))\n",
    "    seqs = model(\n",
    "        ciphered_text_tokenized,\n",
    "        max_output_len=12,\n",
    "        temperature=1e-3\n",
    "    )\n",
    "    deciphered_text = \"\"\n",
    "    seqs = seqs.detach().cpu().numpy().astype(int)\n",
    "    for i in range(len(seqs[0])):\n",
    "        deciphered_text += trg_vocab[\"itos\"][seqs[0][i].item()]\n",
    "    print(\"Original-Deciphered:\", text, \" <--> \", deciphered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[26., 19., 19., 19., 19., 19., 19., 19., 19., 19., 27.,  0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(self, input_tokens, max_output_len, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Forward pass of the seq2seq model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_tokens: Tensor of shape <batch_size x max_length>\n",
    "        The input sequence\n",
    "    max_output_len: int\n",
    "        The maximum length of the output sequence\n",
    "    \"\"\"\n",
    "    batch_size = input_tokens.size(0)\n",
    "    vocab_size = self.vocab_size\n",
    "\n",
    "    # Get encoder outputs and hidden states\n",
    "    print(input_tokens.shape)\n",
    "    encoder_outputs, encoder_hiddens = self.encoder(input_tokens)\n",
    "    decoder_hidden = encoder_hiddens\n",
    "\n",
    "    # First input to decoder is SOS token\n",
    "    decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.sos_token_id\n",
    "\n",
    "    # Initialize outputs tensor to store decoder outputs\n",
    "    outputs = torch.zeros(batch_size, max_output_len, device=self.device)\n",
    "\n",
    "    # Generate sequence\n",
    "    for t in range(max_output_len):\n",
    "        output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        outputs[:, t] = torch.argmax(output, dim=2)\n",
    "\n",
    "        # Sample next token using temperature\n",
    "        if temperature > 0:\n",
    "            probs = torch.nn.functional.softmax(output.squeeze(1) / temperature, dim=-1)\n",
    "            decoder_input = torch.multinomial(probs, 1)\n",
    "        else:\n",
    "            decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "        # Stop if all sequences in batch hit EOS\n",
    "        if (decoder_input == self.eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return outputs\n",
    "forward(model, ciphered_text_tokenized, max_output_len=12, temperature=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
