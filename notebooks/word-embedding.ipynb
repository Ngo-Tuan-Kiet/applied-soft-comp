{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237788b4",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skojaku/applied-soft-comp/blob/master/notebooks/word-embedding.ipynb)\n",
    "\n",
    "# Teaching computers how to understand words\n",
    "\n",
    "Computers only understand numbers, not words. So to make computers understand text, we need to convert words into numbers. In this module, we will learn how to do this conversion using word embedding techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238aa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the required packages\n",
    "# !pip install networkx gensim tqdm nltk bokeh faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ff759",
   "metadata": {},
   "source": [
    "\n",
    "## Distributional Hypothesis\n",
    "\n",
    "One associates `cat` with `dog` because they have similar context, while `cat` is more different from `fish` than `dog` because they are in different contexts. This is the core idea of the **distributional hypothesis**.\n",
    "\n",
    "Let's try to organize this information systematically. Imagine creating a giant table where:\n",
    "- Each row represents a word\n",
    "- Each column represents a document\n",
    "- Each cell contains the count of how often that word appears in that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb988384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat chases mice in the garden\",\n",
    "    \"The dog chases cats in the park\",\n",
    "    \"Mice eat cheese in the house\",\n",
    "    \"The cat and dog play in the garden\"\n",
    "]\n",
    "\n",
    "# Create word count matrix\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display as a table\n",
    "df = pd.DataFrame(count_matrix.toarray(), columns=words)\n",
    "df.style.background_gradient(cmap='cividis', axis = None).set_caption(\"Word-Document Count Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977c8d5",
   "metadata": {},
   "source": [
    "Looking at our word-document matrix, something seems off. Words like \"the\" and \"in\" appear frequently in almost every document. Are these words really the most important for understanding document meaning? Let's see what happens if we count how often each word appears across all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = df.sum(axis=0)\n",
    "word_counts_df = pd.DataFrame(word_counts, columns=[\"count\"]).T\n",
    "word_counts_df.style.background_gradient(cmap='cividis', axis = None).set_caption(\"Total appearances of each word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ebb7c",
   "metadata": {},
   "source": [
    "We've discovered two problems with raw word counts:\n",
    "1. Common words like \"the\" and \"in\" dominate the counts, but they tell us little about document content\n",
    "2. Raw frequencies don't tell us how unique or informative a word is across documents\n",
    "\n",
    "Think about it: if a word appears frequently in one document but rarely in others (like \"cheese\" in our example), it's probably more informative about that document's content than a word that appears equally frequently in all documents (like \"the\").\n",
    "\n",
    "This realization leads us to two important questions:\n",
    "1. How can we normalize word frequencies within each document to account for document length?\n",
    "2. How can we adjust these frequencies to give more weight to words that are unique to specific documents?\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** provides elegant solutions to both questions. Let's see how it works in the next section.\n",
    "\n",
    "## The Need for Normalization\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) offers our first practical glimpse into representing words as distributed patterns rather than isolated units.\n",
    "\n",
    "The TF-IDF score for a word $t$ in document $d$ combines two components:\n",
    "\n",
    "$\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\text{TF}(t,d) = \\dfrac{\\text{count of term }t\\text{ in document }d}{\\text{total number of terms in document }d}$\n",
    "\n",
    "$\\text{IDF}(t) = \\log\\left(\\dfrac{\\text{total number of documents}}{\\text{number of documents containing term }t}\\right)$\n",
    "\n",
    "Let's see this distributed representation in action:\n",
    "First, let us consider a simple example with 5 documents about animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74781c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents about animals\n",
    "documents = [\n",
    "    \"Cats chase mice in the garden\",\n",
    "    \"Dogs chase cats in the park\",\n",
    "    \"Mice eat cheese in the house\",\n",
    "    \"Pets like dogs and cats need care\",\n",
    "    \"Wild animals hunt in nature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa0f9f",
   "metadata": {},
   "source": [
    "Second, we will split each document into words and create a vocabulary.\n",
    "This process is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5800fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "vocab = set()\n",
    "for doc in documents:\n",
    "    tokens_in_doc = doc.lower().split()\n",
    "    tokens.append(tokens_in_doc)\n",
    "    vocab.update(tokens_in_doc)\n",
    "\n",
    "# create a dictionary that maps each word to a unique index\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5a403",
   "metadata": {},
   "source": [
    "Third, we will count how many times each word appears in each document. We begin by creating a placeholder matrix `tf_matrix` to store the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate term frequencies for each document\n",
    "n_docs = len(documents) # number of documents\n",
    "n_terms = len(vocab) # number of words\n",
    "\n",
    "# This is a matrix of zeros\n",
    "# with the number of rows equal to the number of words\n",
    "# and the number of columns equal to the number of documents\n",
    "tf_matrix = np.zeros((n_terms, n_docs))\n",
    "\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753ed5a",
   "metadata": {},
   "source": [
    "And then we count how many times each word appears in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_idx, tokens_in_doc in enumerate(tokens):\n",
    "    for word in tokens_in_doc:\n",
    "        term_idx = word_to_idx[word]\n",
    "        tf_matrix[term_idx, doc_idx] += 1\n",
    "\n",
    "for doc_idx in range(n_docs):\n",
    "    tf_matrix[:, doc_idx] = tf_matrix[:, doc_idx] / np.sum(tf_matrix[:, doc_idx])\n",
    "\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09879f",
   "metadata": {},
   "source": [
    "Fourth, we calculate the IDF for each word.\n",
    "IDF is defined as the logarithm of the inverse document frequency.\n",
    "Document frequency is the number of documents that contain the word.\n",
    "Note that, if a word appears multiple times in the same document, it should only be counted once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d090fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IDF for each term\n",
    "# let's use tf_matrix to calculate the document frequency\n",
    "doc_freq = np.zeros(n_terms)\n",
    "\n",
    "# Go through each word\n",
    "for term_idx in range(n_terms):\n",
    "\n",
    "    # For each word, go through each document\n",
    "    for doc_idx in range(n_docs):\n",
    "        # If the word appears in the document, increment the document frequency\n",
    "        if tf_matrix[term_idx, doc_idx] > 0:\n",
    "            doc_freq[term_idx] += 1\n",
    "\n",
    "idf = np.log(n_docs / doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc9bf9",
   "metadata": {},
   "source": [
    "Or alternatively, in a more efficient way, we can: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e48dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_freq = np.sum(tf_matrix > 0, axis=1).reshape(-1)\n",
    "idf = np.log(n_docs / doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034ada4",
   "metadata": {},
   "source": [
    "Next, we calculate the TF-IDF matrix. `tf_matrix` is a matrix of `n_terms` by `n_docs`, and `idf` is a vector of length `n_terms`. Remind that the tf-idf is given by\n",
    "\n",
    "$\n",
    "\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)\n",
    "$\n",
    "\n",
    "A naive way to do this (albeit not efficient) is to perform this using for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8cb6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF matrix\n",
    "\n",
    "tfidf_matrix = np.zeros((n_terms, n_docs))\n",
    "for term_idx in range(n_terms):\n",
    "    for doc_idx in range(n_docs):\n",
    "        tfidf_matrix[term_idx, doc_idx] = tf_matrix[term_idx, doc_idx] * idf[term_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822f359",
   "metadata": {},
   "source": [
    "A more efficient way is to use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4297f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = np.diag(idf) @ tf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc02144",
   "metadata": {},
   "source": [
    "where `np.diag(idf)` creates a diagonal matrix with `idf` on the diagonal.\n",
    "\n",
    "A more efficient way is to use einsum, which is a powerful function for performing Einstein summation convention on arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a734c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = np.einsum('ij,i->ij', tf_matrix, idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149cda2",
   "metadata": {},
   "source": [
    "Now, we have the TF-IDF matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8faaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7cfd0",
   "metadata": {},
   "source": [
    "Each row of the TF-IDF matrix is our first attempt at a distributed representation of a word.\n",
    "Words that appear together in the same documents frequently will have similar rows.\n",
    "\n",
    "One can reduce the dimensionality of the representation using dimensionality reduction techniques (e.g., PCA, SVD) while maintaining the structure of the data.\n",
    "This is possible because tf-idf matrix is often low-rank in practice.\n",
    "\n",
    "Let us apply PCA to reduce the dimensionality of the tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d47de808",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = PCA(n_components=2)\n",
    "xy = reducer.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7043c2",
   "metadata": {},
   "source": [
    "Let's visualize the result using Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Prepare data for Bokeh\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    text_x=xy[:, 0] + np.random.randn(n_terms) * 0.02,\n",
    "    text_y=xy[:, 1] + np.random.randn(n_terms) * 0.02,\n",
    "    term=list(word_to_idx.keys())\n",
    "))\n",
    "\n",
    "p = figure(title=\"Node Embeddings from Word2Vec\", x_axis_label=\"X\", y_axis_label=\"Y\")\n",
    "\n",
    "p.scatter('x', 'y', source=source, line_color=\"black\", size = 30)\n",
    "\n",
    "# Add labels to the points\n",
    "p.text(x='text_x', y='text_y', text='term', source=source, text_font_size=\"10pt\", text_baseline=\"middle\", text_align=\"center\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b9e17",
   "metadata": {},
   "source": [
    "The power of TF-IDF lies in its ability to transform the distributional hypothesis into a practical mathematical framework. By representing words through their patterns of usage across documents, TF-IDF creates a distributed representation where semantic relationships emerge naturally from the data.\n",
    "\n",
    "However, TF-IDF has its limitations. It only captures word-document relationships, missing out on the rich word-word relationships that occur within documents. This limitation led researchers to develop more sophisticated techniques like word2vec, which we'll explore in the next section.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "With word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)\n",
    "\n",
    "To showcase the effectiveness of word2vec, let’s walk through an example using the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615b671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pre-trained word2vec model from Google News\n",
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc8e8c",
   "metadata": {},
   "source": [
    "This may take some time to load.\n",
    "\n",
    "Our first example is to find the words most similar to king."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f321759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "word = \"king\"\n",
    "similar_words = model.most_similar(word)\n",
    "print(f\"Words most similar to '{word}':\")\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c323e",
   "metadata": {},
   "source": [
    "A cool (yet controversial) application of word embeddings is analogy solving. Let us consider the following puzzle:\n",
    "\n",
    "> man is to woman as king is to ___ ?\n",
    "\n",
    "We can use word embeddings to solve this puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We solve the puzzle by\n",
    "#\n",
    "#  vec(king) - vec(man) + vec(woman)\n",
    "#\n",
    "# To solve this, we use the model.most_similar function, with positive words being \"king\" and \"woman\" (additive), and negative words being \"man\" (subtractive).\n",
    "#\n",
    "model.most_similar(positive=['woman', \"king\"], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fd870",
   "metadata": {},
   "source": [
    "The last example is to visualize the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "countries = ['Germany', 'France', 'Italy', 'Spain', 'Portugal', 'Greece']\n",
    "capital_words = ['Berlin', 'Paris', 'Rome', 'Madrid', 'Lisbon', 'Athens']\n",
    "\n",
    "# Get the word embeddings for the countries and capitals\n",
    "country_embeddings = np.array([model[country] for country in countries])\n",
    "capital_embeddings = np.array([model[capital] for capital in capital_words])\n",
    "\n",
    "# Compute the PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings = np.vstack([country_embeddings, capital_embeddings])\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create a DataFrame for seaborn\n",
    "df = pd.DataFrame(embeddings_pca, columns=['PC1', 'PC2'])\n",
    "df['Label'] = countries + capital_words\n",
    "df['Type'] = ['Country'] * len(countries) + ['Capital'] * len(capital_words)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a scatter plot with seaborn\n",
    "scatter_plot = sns.scatterplot(data=df, x='PC1', y='PC2', hue='Type', style='Type', s=200, palette='deep', markers=['o', 's'])\n",
    "\n",
    "# Annotate the points\n",
    "for i in range(len(df)):\n",
    "    plt.text(df['PC1'][i], df['PC2'][i] + 0.08, df['Label'][i], fontsize=12, ha='center', va='bottom',\n",
    "             bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))\n",
    "\n",
    "# Draw arrows between countries and capitals\n",
    "for i in range(len(countries)):\n",
    "    plt.arrow(df['PC1'][i], df['PC2'][i], df['PC1'][i + len(countries)] - df['PC1'][i], df['PC2'][i + len(countries)] - df['PC2'][i],\n",
    "              color='gray', alpha=0.6, linewidth=1.5, head_width=0.02, head_length=0.03)\n",
    "\n",
    "plt.legend(title='Type', title_fontsize='13', fontsize='11')\n",
    "plt.title('PCA of Country and Capital Word Embeddings', fontsize=16)\n",
    "plt.xlabel('Principal Component 1', fontsize=14)\n",
    "plt.ylabel('Principal Component 2', fontsize=14)\n",
    "ax = plt.gca()\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1f493",
   "metadata": {},
   "source": [
    "### 🔥 Exercise 🔥\n",
    "\n",
    "1. Using the word2vec model, find the 5 most similar words to “computer” and “science”. What do you observe about the semantic relationships between these words?\n",
    "2. Perform the following word analogy tasks using word2vec and explain your findings:\n",
    "  - man : woman :: king : ?\n",
    "  - Paris : France :: Tokyo : ?\n",
    "  - car : cars :: child : ?\n",
    "3. Create a visualization similar to the country-capital example above but using:\n",
    "  - Different professions and their typical workplaces (e.g., doctor-hospital, teacher-school)\n",
    "  - Different languages and their countries (e.g., Spanish-Spain, French-France)\n",
    "4. Advanced: Investigate the concept of “gender bias” in word embeddings:\n",
    "  - Create a visualization similar to the country-capital example above but using the words \"he-she\", \"man-woman\", \"king-queen\"\n",
    "  - Train the PCA on these words and use the trained PCA model to project profession words (e.g., “doctor”, “nurse”, “engineer”, “teacher”) onto these gender directions. What does this tell us about potential biases in the training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3080cd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
