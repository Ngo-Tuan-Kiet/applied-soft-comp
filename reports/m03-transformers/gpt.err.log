Traceback (most recent call last):
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/nbclient/client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/asyncio/base_events.py", line 650, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/contextlib.py", line 222, in __aexit__
    await self.gen.athrow(typ, value, traceback)
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/nbclient/client.py", line 650, in async_setup_kernel
    yield
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/nbclient/client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# We encode the input text into tokens.
model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)

# generate 40 new tokens
greedy_output = model.generate(**model_inputs, max_new_tokens=40)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))
------------------

----- stderr -----
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mIndexError[0m                                Traceback (most recent call last)
Cell [0;32mIn[2], line 5[0m
[1;32m      2[0m model_inputs [38;5;241m=[39m tokenizer([38;5;124m'[39m[38;5;124mI enjoy walking with my cute dog[39m[38;5;124m'[39m, return_tensors[38;5;241m=[39m[38;5;124m'[39m[38;5;124mpt[39m[38;5;124m'[39m)[38;5;241m.[39mto(torch_device)
[1;32m      4[0m [38;5;66;03m# generate 40 new tokens[39;00m
[0;32m----> 5[0m greedy_output [38;5;241m=[39m model[38;5;241m.[39mgenerate([38;5;241m*[39m[38;5;241m*[39mmodel_inputs, max_new_tokens[38;5;241m=[39m[38;5;241m40[39m)
[1;32m      7[0m [38;5;28mprint[39m([38;5;124m"[39m[38;5;124mOutput:[39m[38;5;130;01m\n[39;00m[38;5;124m"[39m [38;5;241m+[39m [38;5;241m100[39m [38;5;241m*[39m [38;5;124m'[39m[38;5;124m-[39m[38;5;124m'[39m)
[1;32m      8[0m [38;5;28mprint[39m(tokenizer[38;5;241m.[39mdecode(greedy_output[[38;5;241m0[39m], skip_special_tokens[38;5;241m=[39m[38;5;28;01mTrue[39;00m))

File [0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/torch/utils/_contextlib.py:115[0m, in [0;36mcontext_decorator.<locals>.decorate_context[0;34m(*args, **kwargs)[0m
[1;32m    112[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m    113[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mdecorate_context[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m    114[0m     [38;5;28;01mwith[39;00m ctx_factory():
[0;32m--> 115[0m         [38;5;28;01mreturn[39;00m func([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/transformers/generation/utils.py:1828[0m, in [0;36mGenerationMixin.generate[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)[0m
[1;32m   1825[0m batch_size [38;5;241m=[39m inputs_tensor[38;5;241m.[39mshape[[38;5;241m0[39m]
[1;32m   1827[0m device [38;5;241m=[39m inputs_tensor[38;5;241m.[39mdevice
[0;32m-> 1828[0m [38;5;28mself[39m[38;5;241m.[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device[38;5;241m=[39mdevice)
[1;32m   1830[0m [38;5;66;03m# decoder-only models must use left-padding for batched generation.[39;00m
[1;32m   1831[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m[38;5;241m.[39mconfig[38;5;241m.[39mis_encoder_decoder [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torchdynamo_compiling():
[1;32m   1832[0m     [38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`[39;00m
[1;32m   1833[0m     [38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.[39;00m

File [0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/transformers/generation/utils.py:1677[0m, in [0;36mGenerationMixin._prepare_special_tokens[0;34m(self, generation_config, kwargs_has_attention_mask, device)[0m
[1;32m   1671[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m   1672[0m         [38;5;124m"[39m[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.[39m[38;5;124m"[39m
[1;32m   1673[0m     )
[1;32m   1674[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m is_torchdynamo_compiling():  [38;5;66;03m# Checks that depend on tensor-dependent control flow[39;00m
[1;32m   1675[0m     [38;5;28;01mif[39;00m (
[1;32m   1676[0m         eos_token_tensor [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[0;32m-> 1677[0m         [38;5;129;01mand[39;00m isin_mps_friendly(elements[38;5;241m=[39meos_token_tensor, test_elements[38;5;241m=[39mpad_token_tensor)[38;5;241m.[39many()
[1;32m   1678[0m     ):
[1;32m   1679[0m         [38;5;28;01mif[39;00m kwargs_has_attention_mask [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m kwargs_has_attention_mask:
[1;32m   1680[0m             logger[38;5;241m.[39mwarning_once(
[1;32m   1681[0m                 [38;5;124m"[39m[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as [39m[38;5;124m"[39m
[1;32m   1682[0m                 [38;5;124m"[39m[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input[39m[38;5;124m'[39m[38;5;124ms [39m[38;5;124m"[39m
[1;32m   1683[0m                 [38;5;124m"[39m[38;5;124m`attention_mask` to obtain reliable results.[39m[38;5;124m"[39m
[1;32m   1684[0m             )

File [0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/transformers/pytorch_utils.py:325[0m, in [0;36misin_mps_friendly[0;34m(elements, test_elements)[0m
[1;32m    311[0m [38;5;250m[39m[38;5;124;03m"""[39;00m
[1;32m    312[0m [38;5;124;03mSame as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting[39;00m
[1;32m    313[0m [38;5;124;03mtorch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075[39;00m
[0;32m   (...)[0m
[1;32m    321[0m [38;5;124;03m    and False otherwise[39;00m
[1;32m    322[0m [38;5;124;03m"""[39;00m
[1;32m    324[0m [38;5;28;01mif[39;00m elements[38;5;241m.[39mdevice[38;5;241m.[39mtype [38;5;241m==[39m [38;5;124m"[39m[38;5;124mmps[39m[38;5;124m"[39m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torch_greater_or_equal_than_2_4:
[0;32m--> 325[0m     [38;5;28;01mreturn[39;00m elements[38;5;241m.[39mtile(test_elements[38;5;241m.[39mshape[[38;5;241m0[39m], [38;5;241m1[39m)[38;5;241m.[39meq(test_elements[38;5;241m.[39munsqueeze([38;5;241m1[39m))[38;5;241m.[39msum(dim[38;5;241m=[39m[38;5;241m0[39m)[38;5;241m.[39mbool()[38;5;241m.[39msqueeze()
[1;32m    326[0m [38;5;28;01melse[39;00m:
[1;32m    327[0m     [38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045[39;00m
[1;32m    328[0m     [38;5;28;01mreturn[39;00m torch[38;5;241m.[39misin(elements, test_elements)

[0;31mIndexError[0m: tuple index out of range

