
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Breaking the Code: From Enigma to Sequence-to-Sequence Models &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm02-doc-embedding/seq2seq';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The Transformer Architecture: Attention is All You Need" href="transformers.html" />
    <link rel="prev" title="Long Short-Term Memory (LSTM)" href="lstm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Words</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/tf-idf.html">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec_plus.html">GloVe and FastText: Building on Word2Vec’s Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/bias-in-embedding.html">Bias in Word Embeddings</a></li>


<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Make Sense of Documents</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 2: Document Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="doc2vec.html">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Breaking the Code: From Enigma to Sequence-to-Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">The Transformer Architecture: Attention is All You Need</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/applied-soft-comp/gh-pages?urlpath=tree/docs/lecture-note/m02-doc-embedding/seq2seq.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm02-doc-embedding/seq2seq.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m02-doc-embedding/seq2seq.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/m02-doc-embedding/seq2seq.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Breaking the Code: From Enigma to Sequence-to-Sequence Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">Model architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-framework">Mathematical Framework</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitation-of-seq2seq">Limitation of seq2seq</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="breaking-the-code-from-enigma-to-sequence-to-sequence-models">
<h1>Breaking the Code: From Enigma to Sequence-to-Sequence Models<a class="headerlink" href="#breaking-the-code-from-enigma-to-sequence-to-sequence-models" title="Link to this heading">#</a></h1>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/%27bombe%27.jpg/1500px-%27bombe%27.jpg"><img alt="Bombe" src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/%27bombe%27.jpg/1500px-%27bombe%27.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Bombe, the codebreaking machine used by Alan Turing and his team at Bletchley Park.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>During World War II, the German military used a sophisticated encryption device called the Enigma machine. This mechanical marvel could transform readable messages into seemingly random sequences of letters, which could only be deciphered by another Enigma machine set to the same configuration. Breaking this code was considered nearly impossible, since the machine could be configured in millions of different ways, and the settings were changed daily.</p>
<p>Alan Turing and his team at Bletchley Park realized that breaking Enigma required understanding how one sequence (the encrypted message) mapped to another sequence (the original text). Their groundbreaking work not only helped win the war but also laid the foundation for modern computing and, in many ways, foreshadowed one of the most powerful concepts in modern machine learning: <em>sequence-to-sequence transformation</em>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The challenge faced at Bletchley Park was, in essence, a sequence-to-sequence problem: transforming a sequence of encrypted characters back into their original message. While the methods used were very different from today’s neural networks, the fundamental goal was the same.</p>
</div>
<p>Today’s sequence-to-sequence (seq2seq) models <a class="footnote-reference brackets" href="#footcite-sutskever2014sequence" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> tackle similar challenges, though at a far more sophisticated level. Like the codebreakers at Bletchley Park, these models learn to transform one sequence into another, whether it’s translating languages, converting speech to text, or summarizing documents. The key difference is that instead of relying on mechanical rotors and manual computations (which are cool!!), modern seq2seq models use neural networks to learn these transformations automatically from data.</p>
<section id="model-architecture">
<h2>Model architecture<a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h2>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/eff9642693c3d83c497a791e80a34e740874f5cd/assets/seq2seq7.png"><img alt="seq2seq model architecture" src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/eff9642693c3d83c497a791e80a34e740874f5cd/assets/seq2seq7.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">seq2seq model architecture.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>seq2seq models are based on a two-part architecture: the encoder and decoder.</p>
<ul class="simple">
<li><p><strong>Encoder</strong> reads the input sequence and compresses it into a context vector, which captures the meaning and nuances of the input. The encoder processes the input sequence one by one, updating its internal state at each step. The final state becomes a context vector that contains a compressed version of the entire input sequence.</p></li>
<li><p><strong>Decoder</strong> takes this fixed-size context vector and generates a completely new sequence autoregressively, potentially of different length and in a different format altogether.</p></li>
</ul>
</section>
<section id="mathematical-framework">
<h2>Mathematical Framework<a class="headerlink" href="#mathematical-framework" title="Link to this heading">#</a></h2>
<p>Sequence-to-sequence models work by transforming sequences using probabilities. At their core, they ask: “Given this input sequence, what’s the probability of generating each element in the output sequence?” During training, the model learns these probabilities by seeing many examples of input-output pairs. Then during inference, it uses what it learned to generate the most likely output sequence for a new input.</p>
<p>Specifically, seq2seq model learns to model the conditional probability between input sequence <span class="math notranslate nohighlight">\((x_1,...,x_n)\)</span> of length <span class="math notranslate nohighlight">\(n\)</span> and output sequence <span class="math notranslate nohighlight">\((y_1,...,y_m)\)</span> of length <span class="math notranslate nohighlight">\(m\)</span>. This probability is expressed as:</p>
<div class="math notranslate nohighlight">
\[
P(y_1,...,y_m|x_1,...,x_n)
\]</div>
<p>The encoder processes the input sequence to create hidden states through the function <span class="math notranslate nohighlight">\(h_t = f(x_t, h_{t-1})\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> is typically an RNN function like LSTM or GRU. The final hidden state <span class="math notranslate nohighlight">\(h_n\)</span> becomes the context vector <span class="math notranslate nohighlight">\(c\)</span>.</p>
<p>The decoder generates the output sequence by modeling each element as <span class="math notranslate nohighlight">\(P(y_t|y_1,...,y_{t-1},c)\)</span>, effectively decomposing the joint probability using the chain rule:</p>
<div class="math notranslate nohighlight">
\[
P(y_1,...,y_m|x_1,...,x_n) = \prod_{t=1}^m P(y_t|y_1,...,y_{t-1},c)
\]</div>
<p>The decoder updates its hidden state at each timestep <span class="math notranslate nohighlight">\(t\)</span> using <span class="math notranslate nohighlight">\(s_t = g(y_{t-1}, s_{t-1}, c)\)</span>, where <span class="math notranslate nohighlight">\(g\)</span> is another RNN function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The decoder takes two hidden states as input: the previous hidden state <span class="math notranslate nohighlight">\(s_{t-1}\)</span> and the context vector <span class="math notranslate nohighlight">\(c\)</span>. Context vector <span class="math notranslate nohighlight">\(c\)</span> is crucial for decoder to note the information from the encoder.</p>
</div>
</section>
<section id="limitation-of-seq2seq">
<h2>Limitation of seq2seq<a class="headerlink" href="#limitation-of-seq2seq" title="Link to this heading">#</a></h2>
<p>The traditional seq2seq architecture faces several critical limitations.The most significant challenge is the <strong>information bottleneck</strong>: compressing the entire input sequence into a fixed-size context vector <span class="math notranslate nohighlight">\(c\)</span>. This becomes particularly problematic for <em>long sequences</em>, where crucial information may be lost during compression.</p>
<p>A second major limitation is the <strong>long-range dependencies</strong> and the <em>vanishing gradient problem</em>. The model struggles to maintain relationships between distant elements in long sequences, as gradients become increasingly small during backpropagation through time, even with LSTM. This particularly hinders the learning of the earlier parts of input sequences, resulting in degraded performance for longer inputs.</p>
<p>A third limitation is that the model treats all input elements equally when creating the context vector, despite the fact that <em>not all inputs are equally relevant</em> for each output element.</p>
</section>
<section id="attention-mechanism">
<h2>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Link to this heading">#</a></h2>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/6-min.png"><img alt="attention mechanism" src="https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/6-min.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">The attention mechanism. The decoder can now see the output of the encoder at each step. Attention mechanism learns the “attention” the decoder should pay to the encoder at each step.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The attention mechanism solves these limitations by letting the decoder focus on specific parts of the input sequence as needed. Instead of using just one fixed context vector, the decoder can look back at different input elements while generating each output. It does this by calculating attention weights that show how important each input element is at each step. These weights are learned during training, so that the model can automatically figure out which parts of the input matter most when generating the output.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The attention mechanism was first introduced in neural machine translation but has since become a fundamental component in many deep learning architectures, including the transformer model that powers systems like GPT and BERT.</p>
</div>
<p>The attention mechanism works as follows. It first calculates the “unnormalized” attention weights <span class="math notranslate nohighlight">\(e_{tj}\)</span> for each input <span class="math notranslate nohighlight">\(j\)</span> at each step <span class="math notranslate nohighlight">\(t\)</span> using a scoring function <span class="math notranslate nohighlight">\(a\)</span>, which can be a neural network or a simple dot product between the decoder hidden state <span class="math notranslate nohighlight">\(s_{t-1}\)</span> and the encoder hidden state <span class="math notranslate nohighlight">\(h_j\)</span>.</p>
<p>This “unnormalized” attention weights are then normalized using the softmax function to obtain the “normalized” attention weights <span class="math notranslate nohighlight">\(\alpha_{tj}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\alpha_{tj} = \frac{\exp(e_{tj})}{\sum_{k=1}^n \exp(e_{tk})}
\]</div>
<p>The context vector <span class="math notranslate nohighlight">\(c_t\)</span> is then computed as a weighted sum of the encoder hidden states <span class="math notranslate nohighlight">\(h_j\)</span> using the attention weights <span class="math notranslate nohighlight">\(\alpha_{tj}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
c_t = \sum_{j=1}^n \alpha_{tj}h_j
\]</div>
<p>The training process minimizes the negative log-likelihood loss:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\sum_{t=1}^m \log P(y_t|y_1,...,y_{t-1},x_1,...,x_n)
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Training a seq2seq model is a fun but challenging coding exercise that covers many technical topics in deep learning, such as padding, masking, teacher forcing, and more. While we will not cover these topics in this course due to technical complexity, I highly recommend you to try it out on your own.
You can find good tutorials on seq2seq model implementation in <a class="reference external" href="https://jaketae.github.io/study/seq2seq-attention/">this blog</a>, <a class="reference external" href="https://greydanus.github.io/2017/01/07/enigma-rnn/">this blog</a>, and <a class="reference external" href="https://github.com/hkhoont/scale_ai_engima_machine">this repo</a>.</p>
</div>
<div class="docutils container" id="id2">
<aside class="footnote brackets" id="footcite-sutskever2014sequence" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In <em>Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS’14, 3104–3112. Cambridge, MA, USA, 2014. MIT Press.</p>
</aside>
</aside>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m02-doc-embedding"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lstm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Long Short-Term Memory (LSTM)</p>
      </div>
    </a>
    <a class="right-next"
       href="transformers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Transformer Architecture: Attention is All You Need</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">Model architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-framework">Mathematical Framework</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitation-of-seq2seq">Limitation of seq2seq</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>