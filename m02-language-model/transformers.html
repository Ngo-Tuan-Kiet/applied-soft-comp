
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>The Transformer Architecture: Attention is All You Need &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm02-language-model/transformers';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Sequence-to-Sequence Models" href="seq2seq.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word and Document Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/tf-idf.html">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec_plus.html">GloVe and FastText: Building on Word2Vec’s Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/bias-in-embedding.html">Bias in Word Embeddings</a></li>


<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/doc2vec.html">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Language Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 2: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq.html">Sequence-to-Sequence Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">The Transformer Architecture: Attention is All You Need</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/applied-soft-comp/gh-pages?urlpath=tree/docs/lecture-note/m02-language-model/transformers.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm02-language-model/transformers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m02-language-model/transformers.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/m02-language-model/transformers.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Transformer Architecture: Attention is All You Need</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">The Transformer Architecture: Attention is All You Need</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-big-picture">The Big Picture</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The Transformer Architecture: Attention is All You Need</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-multiple-perspectives">Multi-Head Attention: Multiple Perspectives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-matters-encoding-sequential-information">Position Matters: Encoding Sequential Information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feed-forward-networks-individual-word-processing">The Feed-Forward Networks: Individual Word Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-keeping-things-stable">Layer Normalization: Keeping Things Stable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-sequential-to-parallel-processing">From Sequential to Parallel Processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-overview">Visual Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-the-key-innovation">Self-Attention: The Key Innovation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-framework">Mathematical Framework</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-parallel-feature-learning">Multi-Head Attention: Parallel Feature Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-aware-processing">Position-Aware Processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-networks-and-layer-normalization">Feed-Forward Networks and Layer Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection-questions">Reflection Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-transformer-architecture-attention-is-all-you-need">
<h1>The Transformer Architecture: Attention is All You Need<a class="headerlink" href="#the-transformer-architecture-attention-is-all-you-need" title="Link to this heading">#</a></h1>
<p>What if we could process language the way we understand a painting - taking in all elements simultaneously rather than sequentially? This fundamental question led to the development of the Transformer architecture, a breakthrough that revolutionized how artificial intelligence processes sequential data.</p>
<section id="the-big-picture">
<h2>The Big Picture<a class="headerlink" href="#the-big-picture" title="Link to this heading">#</a></h2>
<p>Consider a Transformer as a black box for a moment. At its most basic level, it takes in a sequence of tokens (like words in a sentence) and produces another sequence of tokens. But unlike previous approaches that processed tokens one at a time like a human reading word by word, a Transformer processes all tokens simultaneously. This parallel processing ability is what makes Transformers both faster and more effective at understanding relationships between distant elements in a sequence.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The shift from sequential to parallel processing in Transformers is analogous to how our visual system processes images. When you look at a picture, you don’t scan it pixel by pixel. Instead, you take in the entire image at once and understand the relationships between different elements simultaneously.</p>
</div>
<figure class="align-center" id="transformer-overview">
<a class="reference internal image-reference" href="https://lansinuote.com/img/transformer.png"><img alt="Transformer Overview" src="https://lansinuote.com/img/transformer.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">High-level overview of the Transformer architecture showing parallel processing capability</span><a class="headerlink" href="#transformer-overview" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This parallel processing capability leads to two key advantages. First, it enables significant speedup through parallelization - we can process all relationships simultaneously on modern hardware. Second, it eliminates the “distance problem” - relationships between elements can be captured regardless of how far apart they are in the sequence, as every element has direct access to every other element.</p>
<p>Think of it like a group conversation versus a chain of whispers. In a chain of whispers (like RNNs), information might get distorted as it passes through many people. In a group conversation (like Transformers), everyone can hear and respond to everyone else directly, enabling richer, more accurate communication.</p>
<p>The price we pay for these advantages is increased memory usage - we need to store relationships between all pairs of elements. However, this tradeoff has proven worthwhile, as evidenced by the Transformer’s widespread adoption in state-of-the-art language models.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When considering whether to use a Transformer for your task, the key question isn’t just whether you’re working with sequential data, but whether understanding relationships between distant elements is crucial for your task.</p>
</div>
<p>In the following sections, we’ll zoom in progressively on how Transformers achieve this remarkable capability, starting with how they represent and process information, then examining their core components, and finally diving deep into the mechanisms that make it all work.</p>
<p>This introduction maintains a clear narrative flow while building concepts progressively, using concrete analogies to make abstract concepts more approachable. Would you like me to continue with the next section, or would you prefer to refine this section further?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>The Transformer Architecture: Attention is All You Need<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>The Transformer architecture revolutionized how AI processes text by enabling parallel processing of words. Unlike humans who read sequentially, Transformers can analyze an entire text simultaneously, understanding relationships between all words at once.</p>
<p>The key idea behind Transformers is to get rid of the sequential processing altogether for the sake of efficiency. Instead, it uses “attention” mechanisms to process all words simultaneously.
In transformers, the attention is called “self-attention”, since the attention is paid within the same sentence, unlike the sequence-to-sequence models that pays attention from one sentence to another.</p>
<section id="self-attention">
<h2>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h2>
<p>At its core, self-attention is about relationships. When you read the sentence “The cat sat on the mat because it was tired”, how do you know what “it” refers to? You naturally look back at the previous words and determine that “it” refers to “the cat”. Self-attention works similarly, but does this for every word in relation to every other word, simultaneously.</p>
<p>To compute the attention between words, transformers create three types of vectors—<strong>query, key, and value</strong>—for each word. Each of these vectors are created by a neural network (w/ single linear layer) that takes the input word as input, and outputs a vector of the same dimension as the input.</p>
<p>Think of this like a library system: The Query is what you’re looking for, the Keys are like book titles, and the Values are the actual content of the books. When you search (Q) for a specific topic, you match it against book titles (K) to find the relevant content (V).</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="https://cdn-images-1.medium.com/max/2000/1*moKYjUdtx-uEyYMbhPWbIw.png"><img alt="Attention calculation" src="https://cdn-images-1.medium.com/max/2000/1*moKYjUdtx-uEyYMbhPWbIw.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Key, Query, and Value vectors</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The vector of a word is then transformed into a new vector by the following formula:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> is the query vector, <span class="math notranslate nohighlight">\(K\)</span> is the key vector, and <span class="math notranslate nohighlight">\(V\)</span> is the value vector.</p>
<p>Let us break down the formula as follows:</p>
<ul>
<li><p><strong>Attention matrix</strong>: The vector product <span class="math notranslate nohighlight">\(QK^T\)</span> is a matrix of size <span class="math notranslate nohighlight">\(n \times n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of words in the sentence. This matrix is called *“attention matrix.” Attention matrix shows how much attention each word pays to each other word. For example, given the sentence “The cat sat on the mat. It was tired.”, the attention matrix (after applying the softmax function) is:</p>
<img alt="../_images/73c6715dcd36b2a449ddbab03e653f8852c0814bf302db1a92b8639833b93f38.png" src="../_images/73c6715dcd36b2a449ddbab03e653f8852c0814bf302db1a92b8639833b93f38.png" />
<p>A large value in the matrix means that transformer pays more attention to the word in the column as a relevant context to the row word. In this example, “It” and “tired” have high attention to “cat” (0.5, 0.3), meaning that “It” and “tired” are highly related to “cat”.</p>
</li>
<li><p><strong>Softmax function</strong>: The softmax function is then applied to each row of the matrix, resulting in a matrix of size <span class="math notranslate nohighlight">\(n \times n\)</span> where each element is the probability of the word being the next word in the sentence. The denominator <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span> is a scaling factor that prevents the dot product from becoming too large, which can cause the softmax function to saturate and produce very small gradients.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The softmax function has a peculiar property: it is very sensitive to the scale of the input. If the input is too large, the softmax function will saturate and produce very small gradients. This is why we scale the dot product by <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://www.researchgate.net/publication/348703101/figure/fig5/AS:983057658040324&#64;1611390618742/Graphic-representation-of-the-softmax-activation-function.ppm"><img alt="Softmax activation function" src="https://www.researchgate.net/publication/348703101/figure/fig5/AS:983057658040324&#64;1611390618742/Graphic-representation-of-the-softmax-activation-function.ppm" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Graphic representation of the softmax activation function.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</li>
<li><p><strong>Contextualized vector</strong>: The final step in the attention mechanism involves the value vector <span class="math notranslate nohighlight">\(V\)</span>. After normalizing the attention matrix using softmax, we multiply it with <span class="math notranslate nohighlight">\(V\)</span> to produce the output. This multiplication essentially computes a weighted average of the value vectors, where the weights come from the attention scores, i.e., <span class="math notranslate nohighlight">\(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\)</span>. The result is the <em>contextualized vector</em> of the word in the row, where a word vector (<span class="math notranslate nohighlight">\(V\)</span>) is contextualized by other word vectors through the attention scores. Mathematically, for each word <span class="math notranslate nohighlight">\(i\)</span>, its contextualized vector is:</p>
<div class="math notranslate nohighlight">
\[c_i = \sum_{j=1}^n \alpha_{ij}v_j\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_{ij}\)</span> is the <span class="math notranslate nohighlight">\((i,j)\)</span>-th element of the attention matrix after softmax and <span class="math notranslate nohighlight">\(v_j\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th row of <span class="math notranslate nohighlight">\(V\)</span>. It is the weighted average since:</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^n \alpha_{ij} = 1\]</div>
<p>for all <span class="math notranslate nohighlight">\(i\)</span>, with larger weights being assigned to words that are more relevant to the word in the row.</p>
</li>
</ul>
<section id="multi-head-attention-multiple-perspectives">
<h3>Multi-Head Attention: Multiple Perspectives<a class="headerlink" href="#multi-head-attention-multiple-perspectives" title="Link to this heading">#</a></h3>
<p>Why do we need multiple attention heads? Consider how you understand language. When reading a sentence, you simultaneously process multiple aspects: grammar, context, emotion, etc. Multi-head attention allows the model to do something similar. Each head can focus on different aspects of the relationships between words.</p>
<p>For example, in the sentence “The old man walked his dog”, one attention head might focus on subject-verb relationships (“man-walked”), while another might capture possessive relationships (“man-dog”).</p>
</section>
<section id="position-matters-encoding-sequential-information">
<h3>Position Matters: Encoding Sequential Information<a class="headerlink" href="#position-matters-encoding-sequential-information" title="Link to this heading">#</a></h3>
<p>One challenge with processing all words simultaneously is that word order gets lost. In the sentences “Dog bites man” and “Man bites dog”, the words are identical, but the meaning is completely different! This is where positional encoding comes in.</p>
<p>Instead of learning word positions from scratch, Transformers use a clever mathematical formula using sine and cosine functions. These functions create unique patterns for each position, allowing the model to understand word order without sacrificing parallel processing.</p>
</section>
<section id="the-feed-forward-networks-individual-word-processing">
<h3>The Feed-Forward Networks: Individual Word Processing<a class="headerlink" href="#the-feed-forward-networks-individual-word-processing" title="Link to this heading">#</a></h3>
<p>After words have gathered information from their neighbors through attention, each word goes through its own feed-forward neural network. This is like giving each word a chance to “digest” all the information it has collected. The network is intentionally made wider in the middle (the <code class="docutils literal notranslate"><span class="pre">d_ff</span></code> parameter is typically 4 times larger than <code class="docutils literal notranslate"><span class="pre">d_model</span></code>) to allow for more complex processing.</p>
</section>
<section id="layer-normalization-keeping-things-stable">
<h3>Layer Normalization: Keeping Things Stable<a class="headerlink" href="#layer-normalization-keeping-things-stable" title="Link to this heading">#</a></h3>
<p>Training deep neural networks is like trying to balance a very tall tower – it can become unstable easily. Layer normalization helps stabilize this process by ensuring that the values flowing through the network don’t become too large or too small. It’s like having a thermostat that keeps the temperature (values) within a comfortable range.</p>
</section>
<section id="putting-it-all-together">
<h3>Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h3>
<p>The beauty of Transformers lies in how these components work together:</p>
<ol class="arabic simple">
<li><p>Words are first embedded and position-encoded</p></li>
<li><p>Self-attention allows words to gather relevant information from each other</p></li>
<li><p>Multiple attention heads capture different types of relationships</p></li>
<li><p>Feed-forward networks process this gathered information</p></li>
<li><p>Layer normalization keeps everything stable</p></li>
<li><p>Residual connections ensure that no important information is lost</p></li>
</ol>
<p>This architecture has proven so successful that it’s now the foundation for models like BERT, GPT, and many others that have revolutionized natural language processing.</p>
</section>
</section>
<section id="from-sequential-to-parallel-processing">
<h2>From Sequential to Parallel Processing<a class="headerlink" href="#from-sequential-to-parallel-processing" title="Link to this heading">#</a></h2>
<p>Consider this challenge: How can we process a sequence without processing it… sequentially? Our previous models (RNNs, LSTMs) processed tokens one by one, maintaining a hidden state. But this sequential nature had two major drawbacks:</p>
<ol class="arabic simple">
<li><p>Limited parallelization: We had to wait for each step to complete before processing the next token</p></li>
<li><p>Difficulty capturing long-range dependencies: Even with LSTM’s gating mechanisms, information could still get diluted over long sequences</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Transformer architecture, introduced in the “Attention is All You Need” paper (Vaswani et al., 2017), was revolutionary because it showed that we can process sequences entirely through attention mechanisms, without any recurrence or convolution.</p>
</div>
<p>Let’s build our intuition about how this works.</p>
</section>
<section id="visual-overview">
<h2>Visual Overview<a class="headerlink" href="#visual-overview" title="Link to this heading">#</a></h2>
<p>Before we dive into the implementation, let’s visualize the key components:</p>
<figure class="align-center" id="transformer-architecture">
<img alt="Transformer Architecture" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/Transformer-neural-network-11.png" />
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">The Transformer architecture showing encoder and decoder components</span><a class="headerlink" href="#transformer-architecture" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="self-attention-the-key-innovation">
<h2>Self-Attention: The Key Innovation<a class="headerlink" href="#self-attention-the-key-innovation" title="Link to this heading">#</a></h2>
<p>Imagine you’re reading a sentence and trying to understand the meaning of each word. Instead of processing words strictly in order, you look at all words simultaneously and figure out how they relate to each other. This is essentially what self-attention does!</p>
<section id="mathematical-framework">
<h3>Mathematical Framework<a class="headerlink" href="#mathematical-framework" title="Link to this heading">#</a></h3>
<p>For each position in the sequence, we compute three vectors:</p>
<ul class="simple">
<li><p>Query (<span class="math notranslate nohighlight">\(\mathbf{q}\)</span>): What information we’re looking for</p></li>
<li><p>Key (<span class="math notranslate nohighlight">\(\mathbf{k}\)</span>): What information this position offers</p></li>
<li><p>Value (<span class="math notranslate nohighlight">\(\mathbf{v}\)</span>): The actual content at this position</p></li>
</ul>
<p>These are computed using learned weight matrices:</p>
<div class="math notranslate nohighlight">
\[\mathbf{q} = \mathbf{x}W^Q\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{k} = \mathbf{x}W^K\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{v} = \mathbf{x}W^V\]</div>
<p>The attention scores are then computed as:</p>
<div class="math notranslate nohighlight">
\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}\]</div>
<p>Let’s implement this in Python:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># d_model is the dimension of our input/output vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

        <span class="c1"># These linear layers transform our input into Q, K, V vectors</span>
        <span class="c1"># Input shape: (d_model) -&gt; Output shape: (d_model)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x shape: (batch_size, seq_len, d_model)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="c1"># Step 1: Create Q, K, V vectors</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, d_model)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, d_model)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, d_model)</span>

        <span class="c1"># Step 2: Calculate attention scores</span>
        <span class="c1"># matmul(Q, K^T) -&gt; Shape: (batch_size, seq_len, seq_len)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Scale the scores to prevent softmax saturation</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Step 3: Apply softmax to get attention weights</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Step 4: Multiply attention weights with values</span>
        <span class="c1"># Shape: (batch_size, seq_len, d_model)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When implementing self-attention, watch out for the scaling factor <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>. Without it, the dot products can grow large in magnitude, pushing the softmax into regions with small gradients.</p>
</div>
</section>
</section>
<section id="multi-head-attention-parallel-feature-learning">
<h2>Multi-Head Attention: Parallel Feature Learning<a class="headerlink" href="#multi-head-attention-parallel-feature-learning" title="Link to this heading">#</a></h2>
<p>One key insight of Transformers is that a single attention mechanism might be too constraining. Why not let the model attend to different aspects of the sequence simultaneously?</p>
<p>This leads to Multi-Head Attention, where we run several attention mechanisms in parallel:</p>
<div class="math notranslate nohighlight">
\[\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O\]</div>
<p>where each head is:</p>
<div class="math notranslate nohighlight">
\[\text{head}_i = \text{Attention}(\mathbf{Q}W^Q_i, \mathbf{K}W^K_i, \mathbf{V}W^V_i)\]</div>
<p>Let’s implement this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Ensure d_model is divisible by num_heads</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_model must be divisible by num_heads&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="c1"># Split d_model into num_heads pieces</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1"># Linear layers for Q, K, V, and output projections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="c1"># Step 1: Project input into Q, K, V vectors and split into heads</span>
        <span class="c1"># Original shape: (batch_size, seq_len, d_model)</span>
        <span class="c1"># After view: (batch_size, seq_len, num_heads, d_k)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>

        <span class="c1"># Step 2: Transpose for attention computation</span>
        <span class="c1"># New shape: (batch_size, num_heads, seq_len, d_k)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 3: Compute attention scores and apply attention</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># Step 4: Reshape and project back</span>
        <span class="c1"># Transpose back to (batch_size, seq_len, num_heads, d_k)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># Combine heads: (batch_size, seq_len, d_model)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_linear</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="position-aware-processing">
<h2>Position-Aware Processing<a class="headerlink" href="#position-aware-processing" title="Link to this heading">#</a></h2>
<p>One challenge with pure attention-based architectures is that they have no built-in sense of position - they’re permutation invariant! The solution? Add position information directly to the input embeddings.</p>
<p>The original Transformer uses sinusoidal position encodings:</p>
<div class="math notranslate nohighlight">
\[PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})\]</div>
<div class="math notranslate nohighlight">
\[PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>

    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pe</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The choice of sinusoidal functions is clever because it allows the model to easily attend to relative positions through linear combinations of these position encodings.</p>
</div>
</section>
<section id="feed-forward-networks-and-layer-normalization">
<h2>Feed-Forward Networks and Layer Normalization<a class="headerlink" href="#feed-forward-networks-and-layer-normalization" title="Link to this heading">#</a></h2>
<p>Between attention layers, Transformers use simple feed-forward networks with a special structure:</p>
<div class="math notranslate nohighlight">
\[FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2\]</div>
<p>This is implemented as two linear transformations with a ReLU activation in between:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>To help training, Transformers use Layer Normalization before each sub-layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Attention with residual connection and layer norm</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># Feed-forward with residual connection and layer norm</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="reflection-questions">
<h2>Reflection Questions<a class="headerlink" href="#reflection-questions" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>How does the parallel nature of Transformers affect their computational efficiency compared to RNNs?</p></li>
<li><p>Why might multiple attention heads be better than a single, larger attention mechanism?</p></li>
<li><p>How do the residual connections and layer normalization help with training deep Transformer networks?</p></li>
<li><p>What are the limitations of the Transformer architecture? When might traditional RNNs still be preferable?</p></li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When implementing Transformers, pay special attention to:</p>
<ul class="simple">
<li><p>Proper scaling in attention computation</p></li>
<li><p>Correct reshaping for multi-head attention</p></li>
<li><p>Adding positional encodings before the first layer</p></li>
<li><p>Using residual connections around each sub-layer</p></li>
</ul>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p><strong>Basic Implementation Exercise</strong>
Implement a simplified version of self-attention that works with this input:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code should handle:</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Visualization Exercise</strong>
Write code to visualize attention weights using a heatmap:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot_attention</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    <span class="c1"># Your code here</span>
    <span class="k">pass</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m02-language-model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="seq2seq.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Sequence-to-Sequence Models</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">The Transformer Architecture: Attention is All You Need</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-big-picture">The Big Picture</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The Transformer Architecture: Attention is All You Need</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-multiple-perspectives">Multi-Head Attention: Multiple Perspectives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-matters-encoding-sequential-information">Position Matters: Encoding Sequential Information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feed-forward-networks-individual-word-processing">The Feed-Forward Networks: Individual Word Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-keeping-things-stable">Layer Normalization: Keeping Things Stable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-sequential-to-parallel-processing">From Sequential to Parallel Processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-overview">Visual Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-the-key-innovation">Self-Attention: The Key Innovation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-framework">Mathematical Framework</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-parallel-feature-learning">Multi-Head Attention: Parallel Feature Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#position-aware-processing">Position-Aware Processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-networks-and-layer-normalization">Feed-Forward Networks and Layer Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection-questions">Reflection Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>