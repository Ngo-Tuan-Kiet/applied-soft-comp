# Module 2: Language Models

## What to learn in this module

In this module, we will learn about language models - neural networks that can understand and generate human language. We will learn:
- Recurrent Neural Networks (RNNs) for processing sequential text data
- Long-Short Term Memory (LSTM) networks for capturing long-range dependencies
- The Attention mechanism for focusing on relevant parts of text
- Transformers architecture that revolutionized NLP
- BERT and its bidirectional understanding of context
- SentenceTransformers for generating sentence embeddings
- Flan-T5 for instruction-tuned text generation
- Instruction Embedding for better task adaptation
