{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea779e0",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "We learned about ELMo's approach to the problem of polysemy using bidirectional LSTMs. BERT builds upon its bidirectional approach by using a self-attention mechanism in transformers.\n",
    "BERT has become the leading transformer model for natural language processing tasks like question answering and text classification. Its effectiveness led Google to incorporate it into their search engine to improve query understanding. In this section, we will explore BERT's architecture and mechanisms.\n",
    "\n",
    "```{figure} https://cdn.botpenguin.com/assets/website/BERT_c35709b509.webp\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 50%\n",
    ":align: center\n",
    "\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and layer normalization, wired together with residual connections.\n",
    "The output of each layer is fed into the next layer, and as we go through the layers, the token embeddings get more and more contextualized, reflecting the context more and more, thanks to the self-attention mechanism.\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/372906672/figure/fig2/AS:11431281179224913@1691164535766/BERT-model-architecture.ppm\n",
    ":name: bert_architecture\n",
    ":alt: BERT architecture\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. The position embeddings are added to the token embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "```\n",
    "\n",
    "```{admonition} Which layer of BERT should we use?\n",
    ":class: tip\n",
    "\n",
    "BERT internally generates multiple hierarchical representations of the input sentence. The higher layers of the model capture more abstract and context-sensitive information, while the lower layers capture more local and surface-level information. Which layer to use depends on the task. For example, if we want to do text classification, we should use the output of the last layer. If we are interested in word-level representations, we should use the output of the first layer.\n",
    "```\n",
    "\n",
    "\n",
    "## Special tokens\n",
    "\n",
    "BERT uses several special tokens to represent the input sentence.\n",
    "\n",
    "- [CLS] is used to represent the start of the sentence.\n",
    "- [SEP] is used to represent the end of the sentence.\n",
    "- [MASK] is used to represent the masked words.\n",
    "- [UNK] is used to represent the unknown words.\n",
    "\n",
    "For example, the sentence \"The cat sat on the mat. It then went to sleep.\" is represented as \"[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]\".\n",
    "\n",
    "In BERT, [CLS] token is used to classify the input sentences, as we will see later. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings. {footcite}`reimers2019sentence`\n",
    "\n",
    "## Position and Segment embeddings\n",
    "\n",
    "BERT uses *position* and *segment* embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "\n",
    "- Position embeddings are used to provide the model with information about the position of the tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper {footcite}`vaswani2017attention`, BERT uses learnable position embeddings.\n",
    "\n",
    "\n",
    "- The segment embeddings are used to distinguish the sentences in the input. For example, for the sentence \"The cat sat on the mat. It then went to sleep.\", the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segmend embeddings are also learned during the pre-training process.\n",
    "\n",
    "```{figure} https://i.sstatic.net/thmqC.png\n",
    ":name: bert_position_segment_embeddings\n",
    ":alt: BERT position and segment embeddings\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Position and segment embeddings in BERT. Position embeddings, which are learnable, are added to the token embeddings. Segment embeddings indicate the sentence that the token belongs to (e.g., $E_A$ and $E_B$).\n",
    "```\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "Position embeddings can be either absolute or relative:\n",
    "\n",
    "Absolute position embeddings (like in BERT) directly encode the position of each token as a fixed index (1st, 2nd, 3rd position etc). Each position gets its own unique embedding vector that is learned during training.\n",
    "\n",
    "Relative position embeddings (like sinusoidal embeddings in the original Transformer) encode the relative distance between tokens rather than their absolute positions. For example, they can encode that token A is 2 positions away from token B, regardless of their absolute positions in the sequence. This makes them more flexible for handling sequences of varying lengths.\n",
    "\n",
    "For interested readers, you can read more about the difference between absolute and relative position embeddings in [The Use Case for Relative Position Embeddings – Ofir Press](https://ofir.io/The-Use-Case-for-Relative-Position-Embeddings/) and [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n",
    "```\n",
    "\n",
    "### Pre-training\n",
    "\n",
    "A key aspect of BERT is its pre-training process, which involves two main objectives:\n",
    "\n",
    "- Masked Language Modeling (MLM)\n",
    "- Next Sentence Prediction (NSP)\n",
    "\n",
    "Both objectives are designed to learn the language structure, such as the relationship between words and sentences.\n",
    "\n",
    "#### Masked Language Modeling (MLM)\n",
    "\n",
    "In MLM, the model is trained to predict the original words that are masked in the input sentence. The masked words are replaced with a special token, [MASK], and the model is trained to predict the original words. For example, the sentence \"The cat [MASK] on the mat\" is transformed into \"The cat [MASK] on the mat\". The model is trained to predict the original word \"sat\" in the sentence.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Masked Language Modeling (MLM). A token is randomly masked and the model is trained to predict the original word.\n",
    "```\n",
    "\n",
    "To generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking:\n",
    "\n",
    "- 80% of the time, replace the word with the [MASK] token\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat [MASK] on the mat\"\n",
    "\n",
    "- 10% of the time, replace the word with a random word\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat dog on the mat\"\n",
    "\n",
    "- 10% of the time, keep the word unchanged\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat sat on the mat\"\n",
    "\n",
    "The model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens.\n",
    "\n",
    "During training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations.\n",
    "\n",
    "```{tip}\n",
    "While replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective {footcite}`raffel2020exploring`. It has become an essential component of BERT's pre-training process.\n",
    "```\n",
    "\n",
    "#### Next Sentence Prediction (NSP)\n",
    "\n",
    "\n",
    "```{figure} https://amitness.com/posts/images/bert-nsp.png\n",
    ":name: bert_nsp\n",
    ":alt: BERT NSP\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Next Sentence Prediction (NSP). The model is trained to predict whether two sentences are consecutive or not.\n",
    "```\n",
    "\n",
    "Next Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\n",
    "\n",
    "The input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n",
    "\n",
    "$$\n",
    "\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]}\".\n",
    "$$\n",
    "\n",
    "BERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n",
    "\n",
    "These two objectives help BERT learn the structure of language, such as the relationship between words and sentences.\n",
    "\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "A powerful aspect of BERT is its ability to be fine-tuned on a wide range of tasks with minimal changes to the model architecture. This is achieved through transfer learning, where the pre-trained BERT model is used as a starting point for specific tasks.\n",
    "\n",
    "Consider a hospital that wants to classify patient reviews. Due to privacy concerns, collecting enough data to train a deep learning model from scratch would be difficult. This is where BERT shines - since it's already pre-trained on vast amounts of text data and understands language structure, it can be fine-tuned effectively even with a small dataset of patient reviews. The pre-trained BERT model can be adapted to this specific classification task with only minor architectural changes.\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "You can find many fine-tuned and pre-trained models for various tasks by searching the [Hugging Face model hub](https://huggingface.co/models), with the keyword \"BERT\".\n",
    "```\n",
    "\n",
    "## Variants and improvements\n",
    "\n",
    "**RoBERTa (Robustly Optimized BERT Approach)* {footcite}`liu2019roberta`* improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT's core architecture.\n",
    "\n",
    "**DistilBERT** {footcite}`sanh2019distilbert` focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT's performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\n",
    "\n",
    "**ALBERT** {footcite}`lan2019albert` introduced parameter reduction techniques to address BERT's memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\n",
    "\n",
    "Domain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include **BioBERT** {footcite}`lee2020biobert` for biomedical text, **SciBERT** {footcite}`reimers2019sentence` for scientific papers, and **FinBERT** {footcite}`araci2019finbert` for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\n",
    "\n",
    "**Multilingual BERT (mBERT)** {footcite}`liu2019roberta` was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn't explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications.\n",
    "\n",
    "\n",
    "## Hands on\n",
    "\n",
    "Let us load a pre-trained BERT model and see how it works using a sense disambiguation task. The sense disambiguation task is a task that involves identifying the correct sense of a word in a sentence. For example, given a sentence with word \"apple\", we need to identify whether it refers to the fruit or the technology company.\n",
    "\n",
    "Let us first load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec0fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9558625",
   "metadata": {},
   "source": [
    "We will use [CoarseWSD-20](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20). The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses. Read the [README](https://github.com/danlou/bert-disambiguation/blob/master/data/CoarseWSD-20/README.txt) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aeab0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>4</td>\n",
       "      <td>descent was ported to apple 's power macintosh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>6</td>\n",
       "      <td>it runs in black-and-white on the apple macint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0</td>\n",
       "      <td>apple 's newsstand is a popular version of thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>8</td>\n",
       "      <td>reception the game was very well received on a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>18</td>\n",
       "      <td>the company has a proven track record in devel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0</td>\n",
       "      <td>apple groves foster the production of the trad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>13</td>\n",
       "      <td>career following mit , smith worked at a varie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>16</td>\n",
       "      <td>fourth place was awarded to a cuba postman nam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>36</td>\n",
       "      <td>plants grape ( budō ) are a fruit typically ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>8</td>\n",
       "      <td>following the august 2014 acquisition of beats...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_pos                                           sentence  label\n",
       "2068         4  descent was ported to apple 's power macintosh...      0\n",
       "568          6  it runs in black-and-white on the apple macint...      0\n",
       "793          0  apple 's newsstand is a popular version of thi...      0\n",
       "1638         8  reception the game was very well received on a...      0\n",
       "1950        18  the company has a proven track record in devel...      0\n",
       "588          0  apple groves foster the production of the trad...      1\n",
       "1761        13  career following mit , smith worked at a varie...      0\n",
       "1508        16  fourth place was awarded to a cuba postman nam...      1\n",
       "2263        36  plants grape ( budō ) are a fruit typically ha...      1\n",
       "2203         8  following the august 2014 acquisition of beats...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(focal_word, is_train, n_samples=100):\n",
    "    data_type = \"train\" if is_train else \"test\"\n",
    "    data_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt\"\n",
    "    label_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt\"\n",
    "\n",
    "    data_table = pd.read_csv(\n",
    "        data_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"word_pos\": int, \"sentence\": str},\n",
    "        names=[\"word_pos\", \"sentence\"],\n",
    "    )\n",
    "    label_table = pd.read_csv(\n",
    "        label_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"label\": int},\n",
    "        names=[\"label\"],\n",
    "    )\n",
    "    combined_table = pd.concat([data_table, label_table], axis=1)\n",
    "    return combined_table.sample(n_samples)\n",
    "\n",
    "\n",
    "focal_word = \"apple\"\n",
    "\n",
    "train_data = load_data(focal_word, is_train=True)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84d597",
   "metadata": {},
   "source": [
    "We will use transformers library developed by Hugging Face to define the BERT model. To use the model, we will need:\n",
    "\n",
    "- BERT tokenizer that converts the text into tokens.\n",
    "- BERT model that computes the embeddings of the tokens.\n",
    "\n",
    "We will use the bert-base-uncased model and tokenizer. Let's define the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0340ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval() # set the model to evaluation mode\n",
    "print(model) # Print the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1e954",
   "metadata": {},
   "source": [
    "This prints the model architecture, which shows:\n",
    "\n",
    "1. BertEmbeddings layer that converts tokens into embeddings using:\n",
    "   - Word embeddings (30522 vocab size, 768 dimensions)\n",
    "   - Position embeddings (512 positions, 768 dimensions)\n",
    "   - Token type embeddings (2 types, 768 dimensions)\n",
    "   - Layer normalization and dropout\n",
    "\n",
    "2. BertEncoder with 12 identical BertLayers, each containing:\n",
    "   - Self-attention mechanism with query/key/value projections\n",
    "   - Intermediate layer with GELU activation\n",
    "   - Output layer with layer normalization\n",
    "\n",
    "3. BertPooler that processes the [CLS] token embedding with:\n",
    "   - Dense layer (768 dimensions)\n",
    "   - Tanh activation\n",
    "\n",
    "All layers maintain the 768-dimensional hidden size, except the intermediate layer which expands to 3072 dimensions.\n",
    "\n",
    "With BERT, we need to prepare text in ways that BERT can understand. Specifically, we prepend it with [CLS] and append [SEP]. We will then convert the text to a tensor of token ids, which is ready to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1557e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d46ca0",
   "metadata": {},
   "source": [
    "Let's get the BERT embeddings for the sentence \"Bank is located in the city of London\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7457ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bank is located in the city of London\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe964c97",
   "metadata": {},
   "source": [
    "This produces the following output.\n",
    "**Tokenized text**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e6d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'bank', 'is', 'located', 'in', 'the', 'city', 'of', 'london', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab496d1",
   "metadata": {},
   "source": [
    "**Token IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283e1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2924, 2003, 2284, 1999, 1996, 2103, 1997, 2414,  102]])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c68af8",
   "metadata": {},
   "source": [
    "**Segment IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad980400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58478ab3",
   "metadata": {},
   "source": [
    "Then, let's get the BERT embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7182f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model to return hidden states\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "outputs = model(tokens_tensor, segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda50e2",
   "metadata": {},
   "source": [
    "The output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004ed0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many layers?  13\n",
      "Shape?  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(\"how many layers? \", len(hidden_states))\n",
    "print(\"Shape? \", hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba1c9a",
   "metadata": {},
   "source": [
    "The hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers.\n",
    "\n",
    "So, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\n",
    "\n",
    "Here we will take the average over the last four hidden states for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d25a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "last_four_layers = hidden_states[-4:]\n",
    "# Stack the layers and then calculate mean\n",
    "stacked_layers = torch.stack(last_four_layers)\n",
    "emb = torch.mean(stacked_layers, dim=0)\n",
    "\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acce76f",
   "metadata": {},
   "source": [
    "emb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8532228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs[2]  # Access hidden states from tuple output\n",
    "    # Stack the last 4 layers then take mean\n",
    "    stacked_layers = torch.stack(hidden_states[-4:])\n",
    "    emb = torch.mean(stacked_layers, dim=0)\n",
    "    return emb, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f11b8",
   "metadata": {},
   "source": [
    "Now, let us embed text and get the embeddings of the focal token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37643f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []  # label\n",
    "emb = []  # embedding\n",
    "sentences = []  # sentence\n",
    "\n",
    "def get_focal_token_embedding(text, focal_word_idx):\n",
    "    emb, tokenized_text = get_bert_embeddings(text)\n",
    "    return emb[0][focal_word_idx]  # Access first batch dimension\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    focal_word_idx = row[\"word_pos\"]\n",
    "    _emb = get_focal_token_embedding(text, focal_word_idx)\n",
    "    labels.append(row[\"label\"])\n",
    "    emb.append(_emb)\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8eba93",
   "metadata": {},
   "source": [
    "Finally, let us visualize the embeddings using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c86468",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"c85da6b7-5a6c-4c31-aebd-0bc5ef931f72\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"c85da6b7-5a6c-4c31-aebd-0bc5ef931f72\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"c85da6b7-5a6c-4c31-aebd-0bc5ef931f72\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"c85da6b7-5a6c-4c31-aebd-0bc5ef931f72\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"c85da6b7-5a6c-4c31-aebd-0bc5ef931f72\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"cae6f7cb-9689-4c87-acaf-ef73266a2439\" data-root-id=\"p1004\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"9aef7d87-6500-4568-bf8b-6cedae57c91a\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"width\":700,\"height\":500,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1005\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1006\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1014\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1007\",\"attributes\":{\"text\":\"Word Embeddings Visualization\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1046\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"Xnz+QO0PkL+KHYW9YBDHQPjvaj4mzA3AnOMfQFQ55r7ZOkDAxMy2QG+H7kB2AsW+gnFowFsInEAhHg+/9PlRwPez7kD2AkE97FW/v4629cCg0qq/YfDlQCyupUD0pvi+hhcCQEOOz8BV4eXAnVcbQKdrVr9BeanArSM5wBN9pT4+0SO/jrCqwG+CQsAsuig/LgI/wMs9aD8JNSfAHkgFQTRNVL+GrdpAwD5HwGE4LcCg2ZZABWmQwN4hgEB6TynAkhScwDiSVMA0NEPASwmmv7S1iUBrCXFAUQFPwEd4vD14leY+h/WeP3OhwEAINdBA6hRSwKSYe8BYE3TAYIQ/QChQzkD7p9C+Oc8fP7vd9UCyaSRAYRm7v90fj8BwKJPA2yYEQK1nJUCRCag9pCHFQDTWKUCcjb3AoLSkwBT4B8CxsBo/81anwNSBrkDW5rTAggEBvx/Ujj7lQabAZAiFPY6rRr7/dXa/F5I2PxXx/b292YdAW6vXPiHiKL95rrW/zAoqwMqjA79hM5XA2WMeQA==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6Jh7vwD9Ez7RMKPAN7KWPqIXgkD/KHDAOwt7PzgFUr7RnhjAeVnGPxcOx78RhWvABk4dQF8dCj/QGY3Aj7ZXwKzrCsALVRJBW6rQP/fThcChw4A/2cmAPskOnr++8hu/uJDOQDDvM8ALKwbAddIBQJN6UECiRBVBGyFywKWhq8Ay98xAI10RwIKQXcCqtN1AShqcwOpIv785EILA0Y/7v2HqwL9VVs6+xhoePhhiXkAvrbc+MqrHQA0N0L9P6H/AUm9ovzCup8Byzau/HeEgQMyvuEDLEXtAbKhywD5plcBDqw9AOok+wEx94z/2S6A/lecjPx0Xk8CsKpZABdeVwNhdiT8LyUa/0q6TwMOaXcDf5GM/KfZKPnUpzT098vtAHwGivvZRTkBv6WjA7bUuwP0INz/23kTAGdEPQS3s2UBNKgfASrjaQOIEPEBtkb0/IlJ/wDLEAL/0t4DAK3WhP+s4FEGSZKA/OArTwDUGdsBUqK+/GFYFQFjshMCZgDnA3/WyP1F+7r8Q4du9leS3QA==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"label\",[0,0,0,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,1,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,1,1,0,0,1,0,0,0,1,1,1,0,0,0,0,1,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0]],[\"sentence\",[\"descent was ported to apple 's power macintosh in 1996 and both versions support multiplayer network play over a variety of protocols .\",\"it runs in black-and-white on the apple macintosh line of computers .\",\"apple 's newsstand is a popular version of this , but as they do n't allow pornographic material , specific digital newsstands for pornographic magazines exist .\",\"reception the game was very well received on apple app store , boasting a 4\\u00a01/2 out of 5 stars .\",\"the company has a proven track record in developing and designing mainstream applications for the app store ( apple ) , google play , amazon appstore and samsung galaxy apps .\",\"apple groves foster the production of the traditional alcoholic drink , a natural cider ( sidra ) .\",\"career following mit , smith worked at a variety of start-ups , including apple in tokyo and general magic located in mountain view , california , as product design lead on nascent smartphone technologies before she got involved with the launch of planet out in 1995 .\",\"fourth place was awarded to a cuba postman named felix carbajal , despite falling ill to apple he ate from an orchard en route .\",\"plants grape ( bud\\u014d ) are a fruit typically harvested in autumn nashi pear ( \\u68a8 nashi ) , chaenomeles ( boke no mi ) , peach ( momo ) , persimmon ( kaki ) , apple ( ringo ) and grape ( bud\\u014d ) are examples of fruit that are used as autumn kigo .\",\"following the august 2014 acquisition of beats by apple , a new ad was released featuring the pill characters , in which siri refuses to invite them attend a party being held by dr. dre to celebrate the deal .\",\"push notifications push notifications were first introduced to smartphones by apple with the advent of the iphone in 2007 .\",\"apple and microsoft , which between them , support the iso / iec - defined formats aac and the older mp3 .\",\"gehakte hering ( chopped herring ) , a popular appetizer on shabbat , is made by chopping skinned , boned herring with hard-boiled egg , onion , apple , sugar , pepper , and a dash of vinegar .\",\"lx86 was based on the quicktransit dynamic translator from transitive , the same that apple uses for its rosetta emulation layer that enables mac os x to run unmodified powerpc binaries on their intel-based macintoshes .\",\"the hickory area is additionally marketed as a data-center corridor and is home to large data-centers operated by apple and google .\",\"books in english : nemi ( titan books , 2007 , isbn 978-1-84576-586-6 ) nemi ii ( titan books , 2008 , isbn 978-1-84576-614-6 ) nemi iii ( titan books , 2009 , isbn 1-84576-615-6 & isbn 978-1-84576-615-3 ) nemi iv ( titan books , 2010 , isbn 1-84576-589-3 & isbn 978-1-84576-589-7 ) mobile nemi is also available on apple mobile and android devices .\",\"in 1991 -- 1992 , official versions appeared for the apple macintosh and nintendo 's nes , snes , and game boy .\",\"the giant data centers that have been and are being built by firms such as google , apple , amazon and others are far larger and generally much more efficient than the data centers used by most large enterprises .\",\"the escarpment of annonay is surrounded by plateaux and gentle hills used for cultivating cherries , apricot , apple , pear , and other crops .\",\"the economy of the village is basically agrarian , with the cultivation on its irrigated and unirrigated land of almond , cereal , pear , apple , olive , peach , etc. arbeca is situated in the north-northeast of les garrigues , near the comarques of pla d'urgell and urgell , and about 30 kilometers from lleida , the capital of the province of the same name .\",\"in this case it is powerpc and power architecture , processor architectures initially developed in the early 1990s by the aim alliance , i.e. ibm , motorola and apple .\",\"the mac nc , sometimes referred to as the macintosh nc , was a network thin client that was expected to be released by apple by april 1998 .\",\"the apple plaintalk microphone jack used on some older macintosh systems is designed to accept an extended 3.5 mm three-conductor phone connector ; in this case , the tip carries power for a preamplifier inside the microphone .\",\"his business suffered badly when the codling moth appeared in the state , devastating the apple crop .\",\"many online music stores , such as apple 's itunes store , and e-book publishers also use drm , as do cable and satellite service operators , to prevent unauthorized use of content or services .\",\"dairy products were consumed in the form of cheese ( particularly feta ) , and nuts and fruits such as dates , fig , grape , pomegranate , and apple .\",\"kaempferol tea , strawberries , gooseberries , cranberries , grapefruit , apple , pea , brassica ( broccoli , kale , brussels sprouts , cabbage ) , chive , spinach , endive , leek , tomato .\",\"'' on august 24 of that same year , dell announced that they also discontinued the dj ditty in the face of competition from apple , manufacturer of the ipod , and other mp3 player manufacturers .\",\"it can be used to make a german style light white wine with apple and pear flavors .\",\"host plants host plants used by the caterpillar include cabbage , cotton , walnuts , apple , tobacco , pea , potato , clovers , and maize .\",\"hongfujin precision industry co. , a subsidiary of foxconn , is a company which manufactures apple 's iphone 5 , ipod as well as other products for multinational corporations .\",\"apple has been the primary driver of thunderbolt adoption through 2011 , though several other vendors have announced new products and systems featuring thunderbolt .\",\"commercials wme 's commercials division helps partner clients with notable brands like at&t , apple , dior , samsung , gucci , mercedes , pepsi and absolut vodka .\",\"fruit apple , plum , and pear , which grow well in lithuania , are the most commonly used fruit .\",\"discovery is a dessert apple cultivar .\",\"intel and microsoft built their first overseas research and development centers in israel , and other high-tech multi-national corporations , such as ibm , google , apple , hp , cisco systems , and motorola , have opened r & d facilities in the country .\",\"in 1984 doyle , his wife holly , and son rob , developed macpublisher , the first desktop publishing software , for the newly introduced apple macintosh computer .\",\"the music video for '' fuerte '' was directed by richard bernardin , robacho buika and aaron a. the music video has been released by nelstar entertainment to apple 's itunes store on october 22 , 2010 .\",\"by 2012 , apple had announced plans for a server farm south of the dalles in prineville , where facebook already had a similar farm .\",\"time machine , included in apple 's mac os x v10.5 operating system , is not a snapshotting scheme but a system-level incremental backup service : it merely watches mounted volumes for changes and copies changed files periodically to a specially-designated volume using hard links .\",\"'' personal life he married novelist mona simpson , the biological sister of apple founder steve jobs , in 1993 .\",\"a security flaw in apple 's itunes allowed unauthorized third parties to use itunes online update procedures to install unauthorized programs .\",\"other materials that are , or have been , used in the manufacture of dolls include cornhusks , bone , stone , wood , porcelain ( sometimes called china ) , bisque , celluloid , wax , and even apple .\",\"some foods whose processing creates press cakes are olive for olive oil ( pomace ) , peanut for peanut oil , coconut flesh for coconut cream and milk ( sapal ) , grape for wine ( pomace ) , apple for cider ( pomace ) , and soybean for soy milk ( used to make tofu ) ( this is called okara ) or oil .\",\"music video the music video was uploaded to youtube on may 28 , 2014 , using the apple and twitter software vine .\",\"fruit trees such as apple , cherry , peach , apricot and walnut are common in the valley , which is known as '' the fruit bowl of jammu and kashmir '' .\",\"the company produced al gore 's our choice app for the ipad , which won apple 's award for best-designed app in 2011 .\",\"in 2007 , apple announced that tahoma would be bundled with the next version of mac os x v10.5 ( '' leopard '' ) .\",\"description alongside dried pears the filling also contains raisin , walnut and other dried fruit such as apple or figs .\",\"over 100 best buy express zoomshops have rolled out into airports and malls across the u.s. carrying products from apple and many other electronics brands .\",\"eastern europe ( or ashkenazi ) charoset is made from chopped walnuts and apple , spiced with cinnamon and sweet wine .\",\"notable stores include apple , aldo , swarovski , express , claire's , crate & barrel , pottery barn , and red door spa .\",\"on unix - compatible systems , including gnu / linux and apple mac os x , tex is distributed in the form of the tetex distribution and more recently the tex live distribution .\",\"exhibit lighting designs include the entire general motors exhibit at the 2007 and 2008 detroit auto shows or naias as well as the 2005 media event , hewlett packard exhibits at ces and at itu , ibm and apple exhibits at comdex , an at & t exhibit at epcot , and temporary exhibits for porsche , universal pictures and sony hdtv .\",\"common rust fungi in agriculture puccinia sorghi causes common rust in corn gymnosporangium juniperi-virginianae ( cedar-apple rust ) ; the juniperus virginiana is the primary ( telial ) host and the apple , pear or hawthorn is the secondary ( aecial ) host .\",\"some eee pc lines such as the 1000he and 1215s uses the island-style keyboard , similar to keyboards used in apple computers and sony 's vaio series , where the keys are reminiscent of scrabble tiles , being spaced apart and raised from the surface below .\",\"sage fruit company is located in the heart of apple country in the pacific northwest .\",\"on march 30 , 2011 , agon online announced due to the competition from openfeint , plus+ and apple game center , it would shut down on june 30 , 2011 and delete all user data .\",\"it mainly competed against windows 98 and windows 2000 by microsoft , plus mac os 9 by apple .\",\"common hardware reference platform ( chrp ) was a standard system architecture for powerpc - based computer systems published jointly by ibm and apple in 1995 .\",\"today the arboretum contains about 2000 trees , as well as notable collections of shrubs , four hundred varieties of heritage apple and pear trees , and three hundred varieties of lilac .\",\"industry donnybrook is the home of western australia 's apple industry .\",\"fruit trees of apple , peach and apricot were also reported ( but the fruits did not ripen in lhasa ) and also poplar trees and bamboo .\",\"in june , 2010 , boarddocs quick adoption of support for the ipad has generated attention for the product on many apple - related news sites .\",\"in the wake of the mass shooting at a theater in aurora , colorado on july 20 , the trailer was pulled from most theaters running before films and airing on television , and removed from apple 's trailer site and youtube due to a scene in which characters shoot submachine gun at moviegoers through the screen of grauman 's chinese theatre .\",\"while more famous for spreading apple seeds throughout the east , chapman was also a swedenborgian missionary and helped spread this faith among the early settlers around urbana .\",\"apple 's ios ; used by the iphone and ipad lines , utilizes a similar unlock mechanism using an on-screen slider widget to the right .\",\"ifart mobile is an application for the apple ios , distributed using the app store .\",\"it was originally designed to offer a better text-entry experience and alternative input methods not found in apple 's built-in set or suit better the needs for windows '' switchers .\",\"di montefiascone as mildly aromatic with apple notes and high acidity .\",\"origins the clafoutis comes from the limousin region of france , and while black cherries are traditional , there are numerous variations using other fruits , including red cherries , plum , prune , apple , pear , cranberries or blackberries .\",\"a market hall was built in 1884 for the sale of cereal , chestnut , apple and nut .\",\"google 's involvement in the yahoo-microsoft case is also explored as well as the gradually deteriorating relationship between google and apple .\",\"this change was a response to the introduction of hulu and to apple 's new video rental services .\",\"availability while initially available only for macs with a superdrive , it was included until 2011 with all new macs ; from idvd 6 onwards , apple supports the ability to burn projects with third-party optical drives .\",\"it was originally developed for the apple macintosh and released in 1986 .\",\"this integrated unit resembled the original mac series , albeit redesigned to apple 's '' neoclassical '' design language of the era .\",\"the larvae feed on crataegus , apple , pear and cherry .\",\"in the eastern us , the gypsy moth prefers oaks , aspen , apple , sweetgum , speckled alder , basswood , gray , paper birch , poplar , willow , and hawthorns , amongst other species .\",\"flavors there are a variety of flavors , including chicken , lower sodium chicken , cornbread , pork , beef , savory herbs , traditional sage , tomato & onion , san francisco sourdough , mushroom & onion , long grain & wild rice and roasted garlic , turkey , apple and cranberry .\",\"sente works with a number of word processors , including microsoft word , pages ( apple ) , mellel , nisus writer , openoffice.org and others .\",\"cherry , apple , pear , peach and apricot trees are available .\",\"during this time , john traveled to silicon valley and gave a demonstration of the program to engineers at apple and russell brown , art director at adobe .\",\"aside from these two primary crops , the region produces a wide variety of products which include barley , corn , flax , grape , potato , rice , sugar beet , sunflower , tobacco , apricot , pear , plum , apple , cherries , pomegranate , melon , dates , fig , sesame , pistachio , and nuts .\",\"world wide web browser software , such as microsoft 's internet explorer , mozilla firefox , opera , apple 's safari , and google chrome , lets users navigate from one web page to another via hyperlinks embedded in the documents .\",\"'' apple : '' we have never heard of prism .\",\"some traditional turkish desserts are fruit-based : ayva tatl\\u0131s\\u0131 ( quince ) , incir tatl\\u0131s\\u0131 ( fig ) , kabak tatl\\u0131s\\u0131 ( pumpkin ) , elma tatl\\u0131s\\u0131 ( apple ) and armut tatl\\u0131s\\u0131 ( pear ) .\",\"the native file systems of unix-like systems also support arbitrary directory hierarchies , as do , for example , apple 's hierarchical file system , and its successor hfs+ in classic mac os ( hfs + is still used in mac os x ) , the fat file system in ms-dos 2.0 and later and microsoft windows , the ntfs file system in the windows nt family of operating systems , and the ods-2 ( on-disk structure-2 ) and higher levels of the files-11 file system in openvms .\",\"then sony pictures syndicated the series in a multi-platform footprint including : youtube , hulu , the playstation network , google tv , the bravia network , animax , axn , at&t , sprint , etc. on october 19 , 2012 , apple releases an exclusive and innovative mobile app edition of urban wolf available on its 172 itunes app stores worldwide for iphone , ipod touch and ipad .\",\"because userland jailbreaks exploit holes ( vulnerabilities ) in ios ( not the bootrom , iboot , ibss etc. ) they affect users who are both jailbroken and not jailbroken ( potentially compromising the security of users who have not jailbroken ) , apple patched the exploit on versions 4.0 and 3.2.1 .\",\"in july 2012 , greenbytes acquired the zevo zfs technology for mac , developed by former apple engineer don brady , who then joined the greenbytes ' team .\",\"replay volume normalization with music sales moving towards file-based playback , digital downloads and away from cds , there is a possibility that the loudness war will be blunted by normalization technology such as replaygain and apple 's sound check .\",\"in 2008 , gspn submitted an application to apple 's app store , but despite having similar functionality to other approved applications , was rejected ; the application was later accepted .\",\"in peru it is drunk warm , and apple , guan\\u00e1bana and quince is used instead of lulo .\",\"apple had the software taken down from several servers for violating this law .\",\"jef raskin ( march 9 , 1943 -- february 26 , 2005 ) was an american human\\u2013computer interface expert best known for conceiving and starting the macintosh project for apple in the late 1970s .\",\"the apple , pear , cherry , and plum are the most widely grown and eaten , owing to their adaptability .\",\"igenapps is a mobile app generator and a website where users , without programming skills , can create personal or business applications using their apple or android mobile device .\",\"pl\\u0103cint\\u0103 is a romania traditional pastry resembling a thin , small round or square-shaped cake , usually filled with a soft cheese such as urd\\u0103 or apple .\",\"morrow has also done several commercials , ironically for both microsoft and apple .\"]],[\"color\",[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1047\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1048\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1044\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1045\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1013\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1026\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1028\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1029\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1035\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1034\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1037\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1038\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1039\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Label\",\"@label\"],[\"Sentence\",\"@sentence\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1021\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1022\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1023\"},\"axis_label\":\"PCA 2\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1024\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1016\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1017\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1018\"},\"axis_label\":\"PCA 1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1019\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1020\",\"attributes\":{\"axis\":{\"id\":\"p1016\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1025\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1021\"}}}]}}]}};\n",
       "  const render_items = [{\"docid\":\"9aef7d87-6500-4568-bf8b-6cedae57c91a\",\"roots\":{\"p1004\":\"cae6f7cb-9689-4c87-acaf-ef73266a2439\"},\"root_ids\":[\"p1004\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert list of tensors to numpy array\n",
    "emb_numpy = torch.stack(emb).detach().numpy()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "xy = pca.fit_transform(emb_numpy)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Create data source for Bokeh\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    label=labels,\n",
    "    sentence=sentences\n",
    "))\n",
    "\n",
    "# Create Bokeh figure\n",
    "p = figure(title=\"Word Embeddings Visualization\", x_axis_label=\"PCA 1\", y_axis_label=\"PCA 2\",\n",
    "           width=700, height=500)\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips=[\n",
    "    ('Label', '@label'),\n",
    "    ('Sentence', '@sentence')\n",
    "])\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Create color map for labels\n",
    "import seaborn as sns\n",
    "\n",
    "unique_labels = list(set(labels))\n",
    "color_map = sns.color_palette().as_hex()[0:len(unique_labels)]\n",
    "source.data['color'] = [color_map[label] for label in labels]\n",
    "\n",
    "# Add scatter plot\n",
    "p.scatter('x', 'y', size=12, line_color=\"DarkSlateGrey\", line_width=2,\n",
    "         fill_color='color', source=source)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc74a56",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We have used the last 4 layers of BERT to generate the embeddings of the tokens. Now, let's use the last $k = 1, 2, 3$ layers of BERT to generate the embeddings of the tokens. Then plot the embeddings using PCA.\n",
    "\n",
    "## References\n",
    "\n",
    "```{footbibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}