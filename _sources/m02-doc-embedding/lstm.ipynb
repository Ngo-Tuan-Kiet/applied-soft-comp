{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796fe0e0",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "\n",
    "When rememberig a long story, you would naturally focus on important details while letting less relevant information fade. This selective memory is exactly what Long Short-Term Memory (LSTM) networks aim to achieve in artificial neural networks. While standard RNNs struggle with long-term dependencies due to the vanishing gradient problem, LSTMs offer a sophisticated solution through controlled memory mechanisms.\n",
    "\n",
    "```{figure} ../figs/lstm.jpg\n",
    "---\n",
    "width: 500px\n",
    "name: lstm\n",
    "---\n",
    "\n",
    "LSTM architecture showing the cell state (horizontal line at top) and the three gates: forget gate, input gate, and output gate. The cell state acts as a conveyor belt carrying information forward, while gates control information flow.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "At the heart of an LSTM lies a *memory cell* (or *cell state*, i.e., $c_{t}$) that can maintain information over long periods. Think of this cell as a conveyor belt that runs straight through the network, allowing information to flow forward largely unchanged. This cell state forms the backbone of the LSTM's memory system.\n",
    "\n",
    "\n",
    "```{figure} ../figs/lstm-forget-gate.jpg\n",
    "---\n",
    "width: 400px\n",
    "name: lstm-01\n",
    "align: center\n",
    "---\n",
    "\n",
    "Forget gate. $\\sigma(x_t, h_t)$ decides how much of the previous cell state $c_{t-1}$ to keep. For example, if $\\sigma(x_t, h_t) = 0$, the forget gate will completely forget the previous cell state. If $\\sigma(x_t, h_t) = 1$, the forget gate will keep the previous cell state. $\\sigma$ is the sigmoid function which is bounded between 0 and 1.\n",
    "```\n",
    "\n",
    "**Forget Gate:**\n",
    "The LSTM controls this memory through three specialized neural networks called *gates*. The *forget gate* examines the current input and the previous hidden state to decide what information to remove from the cell state. Like a selective eraser, it outputs values between 0 and 1 for each number in the cell state, where 0 means \"completely forget this\" and 1 means \"keep this entirely.\"\n",
    "\n",
    "\n",
    "```{figure} ../figs/lstm-input-gate.jpg\n",
    "---\n",
    "width: 400px\n",
    "name: lstm-02\n",
    "align: center\n",
    "---\n",
    "\n",
    "Input gate. $\\sigma(x_t, h_t)$ decides how much of the new information (that passes through the tanh function) to add to the cell state. For example, if $\\sigma(x_t, h_t) = 0$, the input gate will completely ignore the new candidate information. If $\\sigma(x_t, h_t) = 1$, the input gate will add the new candidate information to the cell state.\n",
    "```\n",
    "\n",
    "The input gate works together with a candidate memory generator to decide what new information to store. The input gate determines how much of the new candidate values should be added to the cell state, while the candidate memory proposes new values that could be added. This mechanism allows the network to selectively update its memory with new information.\n",
    "\n",
    "\n",
    "```{figure} ../figs/lstm-output-gate.jpg\n",
    "---\n",
    "width: 400px\n",
    "name: lstm-03\n",
    "align: center\n",
    "---\n",
    "Output gate. $\\sigma(x_t, h_t)$ decides how much of the cell state to reveal as output. For example, if $\\sigma(x_t, h_t) = 0$, the output gate will completely hide the cell state. If $\\sigma(x_t, h_t) = 1$, the output gate will reveal the cell state.\n",
    "```\n",
    "\n",
    "Finally, the output gate controls what parts of the cell state should be revealed as output. It applies a filtered version of the cell state to produce the hidden state, which serves as both the output for the current timestep and part of the input for the next timestep.\n",
    "\n",
    "```{note}\n",
    "The key innovation of LSTMs is not just having memory, but having controlled memory. The network learns what to remember and what to forget, rather than trying to remember everything.\n",
    "```\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "The LSTM's operation can be described through a series of equations that work together to process sequential data. The cell state $C_t$ evolves according to:\n",
    "\n",
    "$$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t $$\n",
    "\n",
    "where $f_t$ is the forget gate, $i_t$ is the input gate, and $\\tilde{C}_t$ is the candidate memory. The $\\odot$ symbol represents element-wise multiplication, allowing the gates to control information flow by scaling values between 0 and 1.\n",
    "\n",
    "The gates themselves are neural networks that take the current input $x_t$ and previous hidden state $h_{t-1}$ as inputs:\n",
    "\n",
    "$$ f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f) $$\n",
    "$$ i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i) $$\n",
    "$$ o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) $$\n",
    "\n",
    "The candidate memory is generated similarly:\n",
    "\n",
    "$$ \\tilde{C}_t = \\tanh(W_c[h_{t-1}, x_t] + b_c) $$\n",
    "\n",
    "Finally, the hidden state is produced by:\n",
    "\n",
    "$$ h_t = o_t \\odot \\tanh(C_t) $$\n",
    "\n",
    "While we've covered the basic equations of LSTMs, let's explore the technical details more thoroughly. When we write $[h_{t-1}, x_t]$ in our equations, we're performing vector concatenation, combining the previous hidden state with our current input. This creates a rich representation that helps the network make decisions about its memory. The weight matrices in our equations ($W_f$, $W_i$, $W_o$, and $W_c$) transform this concatenated input into the appropriate dimensions for each gate. For instance, if we have an input dimension of d and a hidden state dimension of h, these weight matrices will have dimensions $h Ã— (h+d)$, ensuring our outputs maintain the correct size throughout the network.\n",
    "\n",
    "## Common Challenges and Solutions\n",
    "\n",
    "While LSTMs are powerful, they come with their own set of challenges. Despite being designed to handle the vanishing gradient problem better than vanilla RNNs, extremely long sequences can still pose difficulties. Practitioners often employ gradient clipping to maintain stable training. Memory consumption can become a bottleneck with very long sequences, but this can be addressed through techniques like truncated backpropagation or sequence chunking.\n",
    "\n",
    "Overfitting is another common challenge, as LSTMs have numerous parameters to tune. To combat this, consider using dropout between LSTM layers, implementing layer normalization, or reducing model size if your task doesn't require the full complexity. Training speed can also be a concern due to the sequential nature of processing. Utilizing mini-batching and GPU acceleration can help, or you might consider using [Gated Recurrent Units (GRUs)](https://en.wikipedia.org/wiki/Gated_recurrent_unit) as a lighter alternative.\n",
    "\n",
    "## Hands-on Implementation\n",
    "\n",
    "Let us implement a simple LSTM model. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fb53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        # Linear layers for gates\n",
    "        combined_dim = input_size + hidden_size\n",
    "        self.forget_gate = nn.Linear(combined_dim, hidden_size)\n",
    "        self.input_gate = nn.Linear(combined_dim, hidden_size)\n",
    "        self.cell_gate = nn.Linear(combined_dim, hidden_size)\n",
    "        self.output_gate = nn.Linear(combined_dim, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Initialize forget gate bias to 1\n",
    "        self.forget_gate.bias.data.fill_(1.0)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        hidden: Tuple[torch.Tensor, torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Process each timestep in the sequence\n",
    "        if input.dim() == 2:\n",
    "            batch_size, input_size = input.size()\n",
    "            seq_length = 1\n",
    "            input = input.unsqueeze(1)  # Add sequence dimension\n",
    "        else:\n",
    "            batch_size, seq_length, input_size = input.size()\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_length, self.i2o.out_features, device=self.device)\n",
    "\n",
    "        # Unpack hidden state\n",
    "        h_t, c_t = hidden\n",
    "\n",
    "        # Ensure hidden states have shape (batch_size, hidden_size)\n",
    "        if h_t.dim() == 1:\n",
    "            h_t = h_t.unsqueeze(0)\n",
    "        if c_t.dim() == 1:\n",
    "            c_t = c_t.unsqueeze(0)\n",
    "\n",
    "        # Match batch sizes\n",
    "        if batch_size != h_t.size(0):\n",
    "            h_t = h_t.expand(batch_size, -1)\n",
    "            c_t = c_t.expand(batch_size, -1)\n",
    "\n",
    "        # Process sequence\n",
    "        for t in range(seq_length):\n",
    "            # Get current input timestep\n",
    "            current_input = input[:, t, :]\n",
    "\n",
    "            # Concatenate input and previous hidden state\n",
    "            combined = torch.cat((current_input, h_t), dim=1)\n",
    "\n",
    "            # Calculate gates\n",
    "            f_t = torch.sigmoid(self.forget_gate(combined))\n",
    "            i_t = torch.sigmoid(self.input_gate(combined))\n",
    "            c_tilde = torch.tanh(self.cell_gate(combined))\n",
    "            o_t = torch.sigmoid(self.output_gate(combined))\n",
    "\n",
    "            # Update cell state and hidden state\n",
    "            c_t = f_t * c_t + i_t * c_tilde\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            # Calculate output for this timestep\n",
    "            outputs[:, t, :] = self.i2o(h_t)\n",
    "\n",
    "        if seq_length == 1:\n",
    "            outputs = outputs.squeeze(1)\n",
    "\n",
    "        return outputs, (h_t, c_t)\n",
    "\n",
    "    def initHidden(self, batch_size: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (\n",
    "            torch.zeros(batch_size, self.hidden_size, device=self.device),\n",
    "            torch.zeros(batch_size, self.hidden_size, device=self.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772eec4d",
   "metadata": {},
   "source": [
    "To showcase the LSTM, let's consider a toy task, i.e., parentheses matching. In this task, we are given a sequence of characters, where each character is either a parenthesis or a regular character. We want to predict whether the parentheses are matched or not. For example, the sequence `(a(b)c)` is valid, while the sequence `(a(b)c` is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4315a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences[0]: fpsp(j)y(bhwe)(cuqh(y)ohj\n",
      "y_valid[0]: 0\n"
     ]
    }
   ],
   "source": [
    "from asctools.dataset import generate_parentheses_dataset\n",
    "\n",
    "sequences, y_valid = generate_parentheses_dataset(n_samples=1000, min_length=25, max_length=25)\n",
    "\n",
    "print(\"sequences[0]:\", sequences[0])\n",
    "print(\"y_valid[0]:\", y_valid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a7660",
   "metadata": {},
   "source": [
    "The LSTM model cannot directly take alphabet as input. Instead, we need to convert the alphabet to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f3c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def to_one_hot(sequence):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz)('\n",
    "    one_hot = torch.zeros((len(sequence), len(alphabet)))\n",
    "    for i in range(len(sequence)):\n",
    "        one_hot[i, alphabet.index(sequence[i])] = 1\n",
    "    return one_hot\n",
    "\n",
    "sequences_one_hot = torch.stack([to_one_hot(sequence) for sequence in sequences], dim = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b116d0f",
   "metadata": {},
   "source": [
    "We also need to convert the target to a tensor of the proper shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6f141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = torch.tensor(y_valid, dtype=torch.long)  # Shape should be (batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381dde6",
   "metadata": {},
   "source": [
    "Now, we are ready to train the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5700f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                                                                                                                         | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 - Loss: 0.0000:   0%|                                                                                                            | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                                                                                                                         | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LSTM.forward() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m lstm\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m RNNTrainer(lstm)\n\u001b[0;32m----> 8\u001b[0m losses \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      9\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39msequences_one_hot, \u001b[38;5;66;03m# This is the input sequence.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     targets\u001b[38;5;241m=\u001b[39my_valid, \u001b[38;5;66;03m# This is the target sequence.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), \u001b[38;5;66;03m# This is the loss function.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, \u001b[38;5;66;03m# This is the maximum number of epochs.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;66;03m# This is the learning rate.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     clip_grad_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;66;03m# This is to prevent the gradient from exploding or vanishing.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/applied-soft-comp/libs/asctools/asctools/rnn_trainer.py:120\u001b[0m, in \u001b[0;36mRNNTrainer.train\u001b[0;34m(self, input_tensors, targets, criterion, optimizer, max_epochs, learning_rate, batch_size, teacher_forcing_ratio, patience, min_delta, hidden_init_func, clip_grad_norm, lr_scheduler, lr_patience, lr_factor, show_progress)\u001b[0m\n\u001b[1;32m    117\u001b[0m hidden \u001b[38;5;241m=\u001b[39m hidden_init_func() \u001b[38;5;28;01mif\u001b[39;00m hidden_init_func \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 120\u001b[0m sequence_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch(\n\u001b[1;32m    121\u001b[0m     input_batch,\n\u001b[1;32m    122\u001b[0m     target_batch,\n\u001b[1;32m    123\u001b[0m     hidden,\n\u001b[1;32m    124\u001b[0m     criterion,\n\u001b[1;32m    125\u001b[0m     teacher_forcing_ratio\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    128\u001b[0m sequence_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/projects/applied-soft-comp/libs/asctools/asctools/rnn_trainer.py:190\u001b[0m, in \u001b[0;36mRNNTrainer._train_batch\u001b[0;34m(self, input_tensor, target_tensor, hidden, criterion, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    187\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hidden_state(batch_size, hidden)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Process the entire input sequence\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m encoder_outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_tensor, hidden, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# For classification tasks (target is 1D)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LSTM.forward() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "from asctools.rnn_trainer import RNNTrainer\n",
    "from torch import nn\n",
    "vocab_size = 28 # 26 characters + 2 parentheses\n",
    "\n",
    "lstm = LSTM(input_size=vocab_size, hidden_size=32, output_size=2)\n",
    "lstm.train()\n",
    "trainer = RNNTrainer(lstm)\n",
    "losses = trainer.train(\n",
    "    input_tensors=sequences_one_hot, # This is the input sequence.\n",
    "    targets=y_valid, # This is the target sequence.\n",
    "    criterion=nn.CrossEntropyLoss(), # This is the loss function.\n",
    "    max_epochs=300, # This is the maximum number of epochs.\n",
    "    learning_rate=0.01, # This is the learning rate.\n",
    "    clip_grad_norm=1.0, # This is to prevent the gradient from exploding or vanishing.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35514da",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`nn.CrossEntropyLoss()` is a loss function that is commonly used for classification tasks. It combines `nn.LogSoftmax()` and `nn.NLLLoss()`. See [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for more details.\n",
    "```\n",
    "\n",
    "Let us confirm that the training loss is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb952af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ebf50",
   "metadata": {},
   "source": [
    "Using the trained LSTM model, we can now evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lstm.eval()\n",
    "eval_sequences, eval_y_valid = generate_parentheses_dataset(n_samples = 300)\n",
    "eval_sequences_one_hot = torch.stack([to_one_hot(sequence) for sequence in eval_sequences], dim = 0)\n",
    "outputs = []\n",
    "for sequence in eval_sequences_one_hot:\n",
    "    hidden = lstm.initHidden()\n",
    "    for i in range(len(sequence)):\n",
    "        output, hidden = lstm(sequence[i], hidden)\n",
    "\n",
    "    # Prediction\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    outputs.append(pred.item())\n",
    "\n",
    "accuracy = np.sum(np.array(outputs) == np.array(eval_y_valid)) / len(eval_y_valid)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d8461",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Exercise ðŸ”¥\n",
    "\n",
    "1. Make the problem more challenging by increasing the sequence length to 100.\n",
    "2. Try using a simple RNN model by importing `RNN` from `asctools.rnn`, and compare the performance with the LSTM model.\n",
    "3. The LSTM model uses a linear layer for producing the output (i.e., `self.i2o`). We can change it to a more complex, powerful function, such as a multilayer perceptron. Try implementing it by using `nn.Sequential`, e.g., `nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size))`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}